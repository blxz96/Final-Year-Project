{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "binding-crime",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:56.082859Z",
     "iopub.status.busy": "2021-02-20T09:04:56.082365Z",
     "iopub.status.idle": "2021-02-20T09:04:58.616402Z",
     "shell.execute_reply": "2021-02-20T09:04:58.616795Z"
    },
    "papermill": {
     "duration": 2.556128,
     "end_time": "2021-02-20T09:04:58.616963",
     "exception": false,
     "start_time": "2021-02-20T09:04:56.060835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import scipy\n",
    "import math\n",
    "from scipy import signal\n",
    "import librosa.display\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "GAMMA = 2.5\n",
    "DATA = 'emodb_MFCC'\n",
    "ROOT = './Dataset/{}'.format(DATA)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "MODEL_PATH = './model/{}_DANN_TDNN-BiLSTM_DAP{}_CV'.format(DATA,GAMMA)\n",
    "NPARR_PATH = './array/{}_DANN_TDNN-BiLSTM-GRU_DAP{}_CV accuracies.npz'.format(DATA,GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "consistent-christianity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:58.658400Z",
     "iopub.status.busy": "2021-02-20T09:04:58.657747Z",
     "iopub.status.idle": "2021-02-20T09:04:58.662059Z",
     "shell.execute_reply": "2021-02-20T09:04:58.662516Z"
    },
    "papermill": {
     "duration": 0.02674,
     "end_time": "2021-02-20T09:04:58.662624",
     "exception": false,
     "start_time": "2021-02-20T09:04:58.635884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to:42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(sd):\n",
    "    np.random.seed(sd)\n",
    "    random.seed(sd)\n",
    "    random.Random(sd)\n",
    "    torch.manual_seed(sd)\n",
    "    torch.cuda.manual_seed(sd)\n",
    "    torch.cuda.manual_seed_all(sd)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(\"Seed set to:{}\".format(sd))\n",
    "# also set worker_init_fn=np.random.seed(0),num_workers=0 in dataloader   \n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-connectivity",
   "metadata": {
    "papermill": {
     "duration": 0.017615,
     "end_time": "2021-02-20T09:04:58.697626",
     "exception": false,
     "start_time": "2021-02-20T09:04:58.680011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. CV Dataset for EMODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "strange-asset",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:58.746861Z",
     "iopub.status.busy": "2021-02-20T09:04:58.740075Z",
     "iopub.status.idle": "2021-02-20T09:04:58.748970Z",
     "shell.execute_reply": "2021-02-20T09:04:58.748566Z"
    },
    "papermill": {
     "duration": 0.033531,
     "end_time": "2021-02-20T09:04:58.749057",
     "exception": false,
     "start_time": "2021-02-20T09:04:58.715526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmoDBDataset2(object):\n",
    "    \"\"\"\n",
    "        Create a Dataset for EmoDB. Each item is a tuple of the form:\n",
    "        (feature, sample_rate, emotion, speaker)\n",
    "    \"\"\"\n",
    "    _emotions = { 'W': 0, 'L': 1, 'E': 2, 'A': 3, 'F': 4, 'T': 5, 'N': 6 } \n",
    "    # W = anger, L = boredom, E = disgust, A = anxiety/fear, F = happiness, T = sadness, N = neutral\n",
    "    \n",
    "    _speaker = {'03': 0, '08': 1, '09': 2,'10': 3,'11': 4,'12': 5,'13': 6,'14': 7,'15': 8,'16': 9}\n",
    "\n",
    "    def __init__(self, root, cv_index, split):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Directory containing the features files\n",
    "            split(string): Either train, validate or test set\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.data = []\n",
    "        self.df = pd.DataFrame(self.data, columns=['Speaker', 'Emotion', 'File'])\n",
    "        self.cv = { 0: (['12','13','03','08','10','14'],['15','09'],['11','16']),\n",
    "                    1: (['15','09','03','08','10','14'],['11','16'],['12','13']),\n",
    "                    2: (['15','09','11','16','10','14'],['12','13'],['03','08']),\n",
    "                    3: (['15','09','11','16','12','13'],['03','08'],['10','14']),\n",
    "                    4: (['11','16','12','13','03','08'],['10','14'],['15','09'])\n",
    "                  }\n",
    "\n",
    "        # Iterate through all audio files\n",
    "        for root, dirs, files in os.walk(root):\n",
    "            for file in files:\n",
    "                \n",
    "#                 Every utterance is named according to the same scheme:\n",
    "#                 Positions 1-2: speaker_id; \n",
    "#                 Positions 3-5: code for text; \n",
    "#                 Position 6: emotion; \n",
    "#                 Position 7: if there are more than two versions, these are numbered a, b, c\n",
    "                \n",
    "#                 03 - male, 31 years old\n",
    "#                 08 - female, 34 years\n",
    "#                 09 - female, 21 years\n",
    "#                 10 - male, 32 years\n",
    "#                 11 - male, 26 years\n",
    "#                 12 - male, 30 years\n",
    "#                 13 - female, 32 years\n",
    "#                 14 - female, 35 years\n",
    "#                 15 - male, 25 years\n",
    "#                 16 - female, 31 years\n",
    "                \n",
    "                if split == 'train':\n",
    "                    if file[0:2] in self.cv[cv_index][0]:\n",
    "                        self.data.append([file[0:2], file[5], file])\n",
    "                elif split == 'validate':\n",
    "                    if file[0:2] in self.cv[cv_index][1]:\n",
    "                        self.data.append([file[0:2], file[5], file])\n",
    "                elif split == 'test':\n",
    "                    if file[0:2] in self.cv[cv_index][2]:\n",
    "                        self.data.append([file[0:2], file[5], file])\n",
    "                else:\n",
    "                    print(\"Error: Split can only be train, validate or test!\")\n",
    "                        \n",
    "                #self.data.append([file[0:2], file[5], file])\n",
    "\n",
    "        # Convert data to pandas dataframe\n",
    "        self.df = pd.DataFrame(self.data, columns=['Speaker', 'Emotion', 'File'])\n",
    "\n",
    "        # Map emotion labels to numeric values\n",
    "        self.df['Emotion'] = self.df['Emotion'].map(self._emotions).astype(np.long)\n",
    "        self.df['Speaker'] = self.df['Speaker'].map(self._speaker).astype(np.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        file_name = os.path.join(self.root, self.df.loc[idx, 'File'])\n",
    "        feature = torch.load(file_name)\n",
    "        emotion = self.df.loc[idx, 'Emotion']\n",
    "        speaker = self.df.loc[idx, 'Speaker']\n",
    "        \n",
    "        # return a tuple instead of a dictionary\n",
    "        sample = (feature,emotion,speaker)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convinced-gender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>03a07Wc.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>08b09Fd.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>13a07Tc.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>16a05Ea.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>13b09Fc.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>16b01La.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>03b02Aa.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>16b01Aa.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>16a07Td.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>16a02Ea.pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Speaker  Emotion        File\n",
       "0          0        0  03a07Wc.pt\n",
       "1          1        4  08b09Fd.pt\n",
       "2          6        5  13a07Tc.pt\n",
       "3          9        2  16a05Ea.pt\n",
       "4          6        4  13b09Fc.pt\n",
       "..       ...      ...         ...\n",
       "324        9        1  16b01La.pt\n",
       "325        0        3  03b02Aa.pt\n",
       "326        9        3  16b01Aa.pt\n",
       "327        9        5  16a07Td.pt\n",
       "328        9        2  16a02Ea.pt\n",
       "\n",
       "[329 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize RavdessDataset\n",
    "emodb_dataset_train = EmoDBDataset2(ROOT,cv_index = 4,split= 'train')\n",
    "emodb_dataset_test = EmoDBDataset2(ROOT,cv_index = 4,split= 'test')\n",
    "emodb_dataset_validate = EmoDBDataset2(ROOT,cv_index = 4, split= 'validate')\n",
    "\n",
    "# To view dataframe, uncomment below: \n",
    "emodb_dataset_train.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "arbitrary-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# TRAIN_BATCH_SIZE = 16\n",
    "# VALIDATE_BATCH_SIZE = len(emodb_dataset_validate)\n",
    "# TEST_BATCH_SIZE = len(emodb_dataset_test)\n",
    "# emodb_train_loader = DataLoader(dataset=emodb_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=2, pin_memory=True)\n",
    "# emodb_validate_loader = DataLoader(dataset=emodb_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=2, pin_memory=True)\n",
    "# emodb_test_loader = DataLoader(dataset=emodb_dataset_test, batch_size= TEST_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "statutory-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(emodb_train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-azerbaijan",
   "metadata": {
    "papermill": {
     "duration": 0.018494,
     "end_time": "2021-02-20T09:04:59.074699",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.056205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Network Architectures and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-fluid",
   "metadata": {
    "papermill": {
     "duration": 0.018207,
     "end_time": "2021-02-20T09:04:59.110880",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.092673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unusual-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TDNN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "                    self, \n",
    "                    input_dim=23, \n",
    "                    output_dim=512,\n",
    "                    context_size=5,\n",
    "                    stride=1,\n",
    "                    dilation=1,\n",
    "                    batch_norm=True,\n",
    "                    dropout_p=0.5\n",
    "                ):\n",
    "        '''\n",
    "        TDNN as defined by https://www.danielpovey.com/files/2015_interspeech_multisplice.pdf\n",
    "        Affine transformation not applied globally to all frames but smaller windows with local context\n",
    "        batch_norm: True to include batch normalisation after the non linearity\n",
    "        \n",
    "        Context size and dilation determine the frames selected\n",
    "        (although context size is not really defined in the traditional sense)\n",
    "        For example:\n",
    "            context size 5 and dilation 1 is equivalent to [-2,-1,0,1,2]\n",
    "            context size 3 and dilation 2 is equivalent to [-2, 0, 2]\n",
    "            context size 1 and dilation 1 is equivalent to [0]\n",
    "        '''\n",
    "        super(TDNN, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.stride = stride\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dilation = dilation\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_norm = batch_norm\n",
    "      \n",
    "        self.kernel = nn.Linear(input_dim*context_size, output_dim)\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        if self.batch_norm:\n",
    "            self.bn = nn.BatchNorm1d(output_dim)\n",
    "        if self.dropout_p:\n",
    "            self.drop = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        input: size (batch, seq_len, input_features)\n",
    "        outpu: size (batch, new_seq_len, output_features)\n",
    "        '''\n",
    "        _, _, d = x.shape\n",
    "        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(self.input_dim, d)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Unfold input into smaller temporal contexts\n",
    "        x = F.unfold(\n",
    "                        x, \n",
    "                        (self.context_size, self.input_dim), \n",
    "                        stride=(1,self.input_dim), \n",
    "                        dilation=(self.dilation,1)\n",
    "                    )\n",
    "\n",
    "        # N, output_dim*context_size, new_t = x.shape\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.kernel(x)\n",
    "        x = self.nonlinearity(x)\n",
    "        \n",
    "        if self.dropout_p:\n",
    "            x = self.drop(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            x = x.transpose(1,2)\n",
    "            x = self.bn(x)\n",
    "            x = x.transpose(1,2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dense-gates",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.155659Z",
     "iopub.status.busy": "2021-02-20T09:04:59.155093Z",
     "iopub.status.idle": "2021-02-20T09:04:59.157363Z",
     "shell.execute_reply": "2021-02-20T09:04:59.156887Z"
    },
    "papermill": {
     "duration": 0.028231,
     "end_time": "2021-02-20T09:04:59.157476",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.129245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#MFCC.shape(1,20,281)\n",
    "class FeatureExtractor(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.TDNN1 = TDNN(input_dim= 20, output_dim=128, context_size=5, dilation=2)\n",
    "        self.TDNN2 = TDNN(input_dim= 128, output_dim=64, context_size=3, dilation=4)\n",
    "        self.LSTM = nn.LSTM(input_size= 64, hidden_size = 64, bidirectional = True, batch_first = True)\n",
    "        self.FC = nn.Linear(in_features = 128, out_features = 256) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.squeeze(x).permute(0,2,1)\n",
    "        x = self.TDNN1(x)\n",
    "        #print(\"TDNN 1 shape: {}\".format(x.shape)) #batch, new sequence length, output features = 32 x 273 x 128\n",
    "        x = self.TDNN2(x)\n",
    "        #print(\"TDNN 2 shape: {}\".format(x.shape)) #batch, new sequence length, output features = 32 x 265 x 64\n",
    "        output, (hn,cn) = self.LSTM(x)\n",
    "        #print(\"LSTM shape: {}\".format(output.shape)) # 32 x 265 x 128\n",
    "        x = self.FC(output)\n",
    "        #print(\"FC shape: {}\".format(x.shape)) # 32, 265, 256\n",
    "        mean = torch.mean(x,1)\n",
    "        stdev = torch.std(x,1)\n",
    "        x = torch.cat((mean,stdev),1)\n",
    "        #print(\"Statistical pooling shape: {}\".format(x.shape)) # 32, 265, 256\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fixed-damage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.199330Z",
     "iopub.status.busy": "2021-02-20T09:04:59.198818Z",
     "iopub.status.idle": "2021-02-20T09:04:59.200531Z",
     "shell.execute_reply": "2021-02-20T09:04:59.200903Z"
    },
    "papermill": {
     "duration": 0.025055,
     "end_time": "2021-02-20T09:04:59.201015",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.175960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionClassifier,self).__init__()\n",
    "        self.label_classifier = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(512, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 7),\n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.label_classifier(x)\n",
    "        return F.softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intelligent-outside",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.242419Z",
     "iopub.status.busy": "2021-02-20T09:04:59.241791Z",
     "iopub.status.idle": "2021-02-20T09:04:59.243665Z",
     "shell.execute_reply": "2021-02-20T09:04:59.244054Z"
    },
    "papermill": {
     "duration": 0.024767,
     "end_time": "2021-02-20T09:04:59.244181",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.219414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpeakerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpeakerClassifier,self).__init__()\n",
    "        self.label_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.label_classifier(x)\n",
    "        return F.softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-legislature",
   "metadata": {
    "papermill": {
     "duration": 0.018735,
     "end_time": "2021-02-20T09:04:59.280907",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.262172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ultimate-coalition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.320584Z",
     "iopub.status.busy": "2021-02-20T09:04:59.320094Z",
     "iopub.status.idle": "2021-02-20T09:04:59.322355Z",
     "shell.execute_reply": "2021-02-20T09:04:59.321901Z"
    },
    "papermill": {
     "duration": 0.023185,
     "end_time": "2021-02-20T09:04:59.322452",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.299267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def domain_adaptation_parameter(p):\n",
    "    lambda_p = 2. / (1. + np.exp(-GAMMA*p)) - 1\n",
    "    return lambda_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "separate-preparation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.404720Z",
     "iopub.status.busy": "2021-02-20T09:04:59.404219Z",
     "iopub.status.idle": "2021-02-20T09:04:59.406382Z",
     "shell.execute_reply": "2021-02-20T09:04:59.405957Z"
    },
    "papermill": {
     "duration": 0.023878,
     "end_time": "2021-02-20T09:04:59.406476",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.382598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_parameters(PATH):\n",
    "    torch.save({\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'emotion_classifier_state_dict': emotion_classifier.state_dict(),\n",
    "                'speaker_classifier_state_dict': speaker_classifier.state_dict(),\n",
    "                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                'emotion_optimizer_state_dict': emotion_optimizer.state_dict(),\n",
    "                'speaker_optimizer_state_dict': speaker_optimizer.state_dict(),\n",
    "                }, PATH)\n",
    "    print(\"Models' parameters and optimisers' parameters saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "amended-warrant",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.445473Z",
     "iopub.status.busy": "2021-02-20T09:04:59.444951Z",
     "iopub.status.idle": "2021-02-20T09:04:59.446848Z",
     "shell.execute_reply": "2021-02-20T09:04:59.447204Z"
    },
    "papermill": {
     "duration": 0.023252,
     "end_time": "2021-02-20T09:04:59.447333",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.424081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_accuracies_and_losses(PATH):\n",
    "    np.savez(PATH, \n",
    "         emoClassLoss = fold_emotion_class_losses,\n",
    "         emoTrain_acc = fold_emotion_training_accuracies, \n",
    "         emoValidate_acc = fold_emotion_validating_accuracies,\n",
    "         spkClassLoss = fold_speaker_class_losses,\n",
    "         spkTrain_acc = fold_speaker_training_accuracies,\n",
    "         spkValidate_acc = fold_speaker_validating_accuracies,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-roads",
   "metadata": {
    "papermill": {
     "duration": 0.018784,
     "end_time": "2021-02-20T09:04:59.484034",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.465250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-elephant",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.545284Z",
     "iopub.status.busy": "2021-02-20T09:04:59.538015Z",
     "iopub.status.idle": "2021-02-20T09:54:22.812226Z",
     "shell.execute_reply": "2021-02-20T09:54:22.812679Z"
    },
    "papermill": {
     "duration": 2963.31054,
     "end_time": "2021-02-20T09:54:22.812897",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.502357",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.optim as optim\n",
    "import pylab as plt\n",
    "\n",
    "# Read this to understand how GRL works: \n",
    "# https://christineai.blog/category/domain-adaptation/\n",
    "\n",
    "FOLDS = 5\n",
    "EPOCHS = 100\n",
    "\n",
    "############### To comment this section out if disrupted #############\n",
    "fold_emotion_class_losses = np.zeros((FOLDS,EPOCHS))\n",
    "fold_emotion_training_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "fold_emotion_validating_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "fold_speaker_class_losses = np.zeros((FOLDS,EPOCHS))\n",
    "fold_speaker_training_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "fold_speaker_validating_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "\n",
    "\n",
    "######### To uncomment if trying to continue disrupted training ######\n",
    "# fold_emotion_class_losses, fold_emotion_training_accuracies, fold_emotion_validating_accuracies， fold_speaker_class_losses，fold_speaker_training_accuracies，fold_speaker_validating_accuracies= np.load(NPARR_PATH)\n",
    "\n",
    "###################################################################\n",
    "\n",
    "# 5-fold cross validation\n",
    "for fold in range(0,FOLDS):\n",
    "\n",
    "    # Selecting CPU or GPU\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    # Selecting the type of encoder, label classifier\n",
    "    encoder = FeatureExtractor().to(DEVICE).train()\n",
    "    emotion_classifier = EmotionClassifier().to(DEVICE).train()\n",
    "    speaker_classifier = SpeakerClassifier().to(DEVICE).train()\n",
    "    \n",
    "#     encoder = nn.DataParallel(encoder)\n",
    "#     emotion_classifier = nn.DataParallel(emotion_classifier)\n",
    "#     speaker_classifier = nn.DataParallel(speaker_classifier)\n",
    "\n",
    "    # Optimizer \n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters())\n",
    "    emotion_optimizer = torch.optim.Adam(emotion_classifier.parameters())\n",
    "    speaker_optimizer = torch.optim.Adam(speaker_classifier.parameters())\n",
    "\n",
    "    cross_entropy_loss = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    \n",
    "    emodb_dataset_train = EmoDBDataset2(ROOT,cv_index = fold ,split= 'train')\n",
    "    emodb_dataset_test = EmoDBDataset2(ROOT,cv_index = fold, split= 'test')\n",
    "    emodb_dataset_validate = EmoDBDataset2(ROOT ,cv_index = fold, split= 'validate')\n",
    "    \n",
    "    # DANN should be trained on labelled data from the source domain and unlabelled data from the target domain\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    emodb_train_loader = DataLoader(dataset=emodb_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=True,worker_init_fn=np.random.seed(42),num_workers=4, pin_memory= True)\n",
    "    \n",
    "    # For evaluation purposes\n",
    "    VALIDATE_BATCH_SIZE = len(emodb_dataset_validate)\n",
    "    emodb_validate_loader = DataLoader(dataset=emodb_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=4, pin_memory= True)\n",
    "    \n",
    "    epoch_emotion_class_losses = []\n",
    "    emotion_training_accuracies = []\n",
    "    emotion_validating_accuracies = []\n",
    "\n",
    "    epoch_speaker_class_losses = []\n",
    "    speaker_training_accuracies = []\n",
    "    speaker_validating_accuracies = []\n",
    "\n",
    "    STEP = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"\\nCurrent Fold: {} | Epoch: {}\".format(fold, epoch))\n",
    "\n",
    "        completed_start_steps = epoch * len(emodb_train_loader)\n",
    "        total_steps = EPOCHS * len(emodb_train_loader)\n",
    "\n",
    "        batch_emotion_class_losses = []\n",
    "        batch_speaker_class_losses = []\n",
    "\n",
    "\n",
    "        for batch_idx, (feature, emotion, speaker) in enumerate(emodb_train_loader):\n",
    "\n",
    "            # Assigned to DEVICE. \n",
    "            features, emotion, speaker = feature.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE)\n",
    "            \n",
    "            # Computing the training progress\n",
    "            p = (batch_idx + completed_start_steps) / total_steps\n",
    "            lambda_p = domain_adaptation_parameter(p)\n",
    "\n",
    "            # Calculate speaker and emotion classification prediction \n",
    "            conv_features = encoder(features)\n",
    "            emotion_preds = emotion_classifier(conv_features)\n",
    "            emotion_class_loss = cross_entropy_loss(emotion_preds, emotion)\n",
    "            speaker_preds = speaker_classifier(conv_features)\n",
    "            speaker_class_loss = cross_entropy_loss(speaker_preds, speaker)\n",
    "\n",
    "            # Calculate total loss\n",
    "            total_loss = emotion_class_loss - lambda_p * speaker_class_loss \n",
    "\n",
    "            # Clear the gradient to prevent gradient accumulation\n",
    "            encoder.zero_grad(set_to_none= True)\n",
    "            emotion_classifier.zero_grad(set_to_none= True)\n",
    "            speaker_classifier.zero_grad(set_to_none= True)\n",
    "\n",
    "            # Computing the gradient\n",
    "            total_loss.backward(retain_graph=True)\n",
    "\n",
    "            # Update the weight\n",
    "            emotion_optimizer.step()\n",
    "            speaker_optimizer.step()\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "            batch_emotion_class_losses.append(emotion_class_loss.detach())\n",
    "            batch_speaker_class_losses.append(speaker_class_loss.detach())\n",
    "\n",
    "\n",
    "        # Enter evaluation mode at the end of each epoch\n",
    "        encoder.eval()\n",
    "        emotion_classifier.eval()\n",
    "        speaker_classifier.eval()\n",
    "\n",
    "        emotion_training_correct, emotion_validating_correct, speaker_training_correct, speaker_validating_correct = 0 , 0 , 0 , 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "        # 1. Training Classification\n",
    "          for index, (features, emotion, speaker) in enumerate(emodb_train_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            emotion_training_correct += (emotion_preds == emotion).sum() \n",
    "            speaker_training_correct += (speaker_preds == speaker).sum() \n",
    "          #source_accuracy = torch.true_divide(source_correct, len(svhn_test_loader.dataset))\n",
    "          emotion_training_accuracy = emotion_training_correct.item()/(len(emodb_train_loader)*TRAIN_BATCH_SIZE)\n",
    "          emotion_training_accuracies.append(emotion_training_accuracy)\n",
    "          speaker_training_accuracy = speaker_training_correct.item()/(len(emodb_train_loader)*TRAIN_BATCH_SIZE)\n",
    "          speaker_training_accuracies.append(speaker_training_accuracy)\n",
    "          print(\"Emotion Training Correct: {}/{} \\nEmotion Training Accuracy: {:.5f}%\".format(emotion_training_correct,(len(emodb_train_loader)*TRAIN_BATCH_SIZE),emotion_training_accuracy*100))\n",
    "          print(\"Speaker Training Correct: {}/{} \\nSpeaker Training Accuracy: {:.5f}%\".format(speaker_training_correct,(len(emodb_train_loader)*TRAIN_BATCH_SIZE),speaker_training_accuracy*100))\n",
    "\n",
    "        # 2. Validating Classification\n",
    "          for index, (features, emotion, speaker) in enumerate(emodb_validate_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            emotion_validating_correct += (emotion_preds == emotion).sum() \n",
    "            speaker_validating_correct += (speaker_preds == speaker).sum() \n",
    "          #source_accuracy = torch.true_divide(source_correct, len(svhn_test_loader.dataset))\n",
    "          emotion_validating_accuracy = emotion_validating_correct.item()/(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "          emotion_validating_accuracies.append(emotion_validating_accuracy)\n",
    "          speaker_validating_accuracy = speaker_validating_correct.item()/(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "          speaker_validating_accuracies.append(speaker_validating_accuracy)\n",
    "          print(\"\\nEmotion Validating Correct: {}/{} \\nEmotion Validating Accuracy: {:.5f}%\".format(emotion_validating_correct,(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE),emotion_validating_accuracy*100))\n",
    "          print(\"Speaker Validating Correct: {}/{} \\nSpeaker Validating Accuracy: {:.5f}%\".format(speaker_validating_correct,(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE),speaker_validating_accuracy*100))\n",
    "\n",
    "          if (len(emotion_validating_accuracies)> 1 and emotion_validating_accuracy >= max(emotion_validating_accuracies[:-1])):\n",
    "                save_parameters(MODEL_PATH + 'fold' + str(fold))\n",
    "\n",
    "\n",
    "        encoder.train()\n",
    "        emotion_classifier.train()\n",
    "        speaker_classifier.train()\n",
    "\n",
    "\n",
    "        epoch_emotion_class_loss = torch.mean(torch.stack(batch_emotion_class_losses), dim=0)\n",
    "        epoch_emotion_class_losses.append(epoch_emotion_class_loss)\n",
    "        epoch_speaker_class_loss = torch.mean(torch.stack(batch_speaker_class_losses), dim=0)\n",
    "        epoch_speaker_class_losses.append(epoch_speaker_class_loss)\n",
    "        \n",
    "    fold_emotion_class_losses[fold] = epoch_emotion_class_losses\n",
    "    fold_emotion_training_accuracies[fold] = emotion_training_accuracies\n",
    "    fold_emotion_validating_accuracies[fold] = emotion_validating_accuracies\n",
    "    fold_speaker_class_losses[fold] = epoch_speaker_class_losses\n",
    "    fold_speaker_training_accuracies[fold] = speaker_training_accuracies\n",
    "    fold_speaker_validating_accuracies[fold] = speaker_validating_accuracies\n",
    "    \n",
    "    save_accuracies_and_losses(NPARR_PATH)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-xerox",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:54:23.326127Z",
     "iopub.status.busy": "2021-02-20T09:54:23.320786Z",
     "iopub.status.idle": "2021-02-20T09:54:23.630414Z",
     "shell.execute_reply": "2021-02-20T09:54:23.630869Z"
    },
    "papermill": {
     "duration": 0.578632,
     "end_time": "2021-02-20T09:54:23.631014",
     "exception": false,
     "start_time": "2021-02-20T09:54:23.052382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Losses vs. epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Losses')\n",
    "\n",
    "for i in range(FOLDS):\n",
    "    plt.plot(range(EPOCHS), fold_emotion_class_losses[i],label='emotion classification loss fold {}'.format(i))\n",
    "    plt.plot(range(EPOCHS), fold_speaker_class_losses[i],label='speaker classification loss fold {}'.format(i))\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-mission",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:54:24.126173Z",
     "iopub.status.busy": "2021-02-20T09:54:24.125448Z",
     "iopub.status.idle": "2021-02-20T09:54:24.126985Z",
     "shell.execute_reply": "2021-02-20T09:54:24.127428Z"
    },
    "papermill": {
     "duration": 0.25619,
     "end_time": "2021-02-20T09:54:24.127572",
     "exception": false,
     "start_time": "2021-02-20T09:54:23.871382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_accuracies_vs_epochs(fold):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.title('Accuracies vs. epochs')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.plot(range(EPOCHS), fold_emotion_training_accuracies[fold],label='emotion_training_accuracies fold {}'.format(fold))\n",
    "    plt.plot(range(EPOCHS), fold_emotion_validating_accuracies[fold],label='emotion_validating_accuracies fold {}'.format(fold))\n",
    "    plt.plot(range(EPOCHS), fold_speaker_training_accuracies[fold],label='speaker_training_accuracies fold {}'.format(fold))\n",
    "    plt.plot(range(EPOCHS), fold_speaker_validating_accuracies[fold],label='speaker_validating_accuracies fold {}'.format(fold))\n",
    "    plt.legend(loc='best')\n",
    "    plt.show() \n",
    "    print(\"Maximum emotion training accuracy:{:.2f}%\".format(max(fold_emotion_training_accuracies[fold])*100))\n",
    "    print(\"Maximum emotion validating accuracy:{:.2f}%\".format(max(fold_emotion_validating_accuracies[fold])*100))\n",
    "    print(\"Maximum speaker training accuracy:{:.2f}%\".format(max(fold_speaker_training_accuracies[fold])*100))\n",
    "    print(\"Maximum speaker validating accuracy:{:.2f}%\".format(max(fold_speaker_validating_accuracies[fold])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-capacity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:54:24.627534Z",
     "iopub.status.busy": "2021-02-20T09:54:24.626950Z",
     "iopub.status.idle": "2021-02-20T09:54:25.636654Z",
     "shell.execute_reply": "2021-02-20T09:54:25.636252Z"
    },
    "papermill": {
     "duration": 1.270501,
     "end_time": "2021-02-20T09:54:25.636773",
     "exception": false,
     "start_time": "2021-02-20T09:54:24.366272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(FOLDS):\n",
    "    plot_accuracies_vs_epochs(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-exploration",
   "metadata": {
    "papermill": {
     "duration": 0.26577,
     "end_time": "2021-02-20T09:54:26.153738",
     "exception": false,
     "start_time": "2021-02-20T09:54:25.887968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Loading and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-involvement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:54:26.715237Z",
     "iopub.status.busy": "2021-02-20T09:54:26.706591Z",
     "iopub.status.idle": "2021-02-20T09:54:37.603591Z",
     "shell.execute_reply": "2021-02-20T09:54:37.604012Z"
    },
    "papermill": {
     "duration": 11.183106,
     "end_time": "2021-02-20T09:54:37.604178",
     "exception": false,
     "start_time": "2021-02-20T09:54:26.421072",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from statistics import mean , stdev\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "lfold_emotion_training_accuracies, lfold_emotion_validating_accuracies, lfold_emotion_testing_accuracies = [] , [] , []\n",
    "lfold_speaker_training_accuracies, lfold_speaker_validating_accuracies, lfold_speaker_testing_accuracies = [] , [] , []\n",
    "\n",
    "for fold in range(5):\n",
    "    print(\"\\nEvaluation for fold {}\".format(fold))\n",
    "    checkpoint = torch.load(MODEL_PATH + 'fold' + str(fold))\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    encoder = FeatureExtractor().to(DEVICE).train()\n",
    "    emotion_classifier = EmotionClassifier().to(DEVICE).train()\n",
    "    speaker_classifier = SpeakerClassifier().to(DEVICE)\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters())\n",
    "    emotion_optimizer = torch.optim.Adam(emotion_classifier.parameters())\n",
    "    speaker_optimizer = torch.optim.Adam(speaker_classifier.parameters())\n",
    "    \n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    emotion_classifier.load_state_dict(checkpoint['emotion_classifier_state_dict'])\n",
    "    speaker_classifier.load_state_dict(checkpoint['speaker_classifier_state_dict'])\n",
    "    \n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    emotion_optimizer.load_state_dict(checkpoint['emotion_optimizer_state_dict'])\n",
    "    speaker_optimizer.load_state_dict(checkpoint['speaker_optimizer_state_dict'])\n",
    "    \n",
    "    emodb_dataset_train = EmoDBDataset2(ROOT,cv_index = fold ,split= 'train')\n",
    "    emodb_dataset_test = EmoDBDataset2(ROOT,cv_index = fold, split= 'test')\n",
    "    emodb_dataset_validate = EmoDBDataset2(ROOT,cv_index = fold, split= 'validate')\n",
    "    \n",
    "    TRAIN_BATCH_SIZE = len(emodb_dataset_train)\n",
    "    VALIDATE_BATCH_SIZE = len(emodb_dataset_validate)\n",
    "    TEST_BATCH_SIZE = len(emodb_dataset_test)\n",
    "    \n",
    "    emodb_train_loader = DataLoader(dataset=emodb_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    emodb_validate_loader = DataLoader(dataset=emodb_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    emodb_test_loader = DataLoader(dataset=emodb_dataset_test, batch_size= TEST_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    \n",
    "    encoder.eval()\n",
    "    emotion_classifier.eval()\n",
    "\n",
    "    lemotion_training_correct, lemotion_validating_correct, lemotion_testing_correct = 0 , 0 , 0\n",
    "    lspeaker_training_correct, lspeaker_validating_correct, lspeaker_testing_correct = 0 , 0 , 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "\n",
    "        # 1. Training Classification\n",
    "        for index, (features, emotion, speaker) in enumerate(emodb_train_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            lemotion_training_correct += (emotion_preds == emotion).sum() \n",
    "            lspeaker_training_correct += (speaker_preds == speaker).sum()\n",
    "        emotion_training_accuracy = lemotion_training_correct.item()/(len(emodb_train_loader)*TRAIN_BATCH_SIZE)\n",
    "        speaker_training_accuracy = lspeaker_training_correct.item()/(len(emodb_train_loader)*TRAIN_BATCH_SIZE)\n",
    "        print(\"\\nEmotion Training Correct: {}/{} \\nEmotion Training Accuracy: {:.5f}%\".format(lemotion_training_correct,(len(emodb_train_loader)*TRAIN_BATCH_SIZE),emotion_training_accuracy*100))\n",
    "        print(\"Speaker Training Correct: {}/{} \\nSpeaker Training Accuracy: {:.5f}%\".format(lspeaker_training_correct,(len(emodb_train_loader)*TRAIN_BATCH_SIZE),speaker_training_accuracy*100)) \n",
    "        \n",
    "        # 2. Validating Classification\n",
    "        for index, (features, emotion, speaker) in enumerate(emodb_validate_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            lemotion_validating_correct += (emotion_preds == emotion).sum() \n",
    "            lspeaker_validating_correct += (speaker_preds == speaker).sum()\n",
    "        emotion_validating_accuracy = lemotion_validating_correct.item()/(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "        speaker_validating_accuracy = lspeaker_validating_correct.item()/(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "        print(\"\\nEmotion Validating Correct: {}/{} \\nEmotion Validating Accuracy: {:.5f}%\".format(lemotion_validating_correct,(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE),emotion_validating_accuracy*100))\n",
    "        print(\"Speaker Validating Correct: {}/{} \\nSpeaker Validating Accuracy: {:.5f}%\".format(lspeaker_validating_correct,(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE),speaker_validating_accuracy*100)) \n",
    "        \n",
    "\n",
    "        # 3. Testing Classification\n",
    "        for index, (features, emotion, speaker) in enumerate(emodb_test_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            lemotion_testing_correct += (emotion_preds == emotion).sum() \n",
    "            lspeaker_testing_correct += (speaker_preds == speaker).sum()\n",
    "        emotion_testing_accuracy = lemotion_testing_correct.item()/(len(emodb_test_loader)*TEST_BATCH_SIZE)\n",
    "        speaker_testing_accuracy = lspeaker_testing_correct.item()/(len(emodb_test_loader)*TEST_BATCH_SIZE)\n",
    "        print(\"\\nEmotion Testing Correct: {}/{} \\nEmotion Testing Accuracy: {:.5f}%\".format(lemotion_testing_correct,(len(emodb_test_loader)*TEST_BATCH_SIZE),emotion_testing_accuracy*100))\n",
    "        print(\"Speaker Testing Correct: {}/{} \\nSpeaker Testing Accuracy: {:.5f}%\".format(lspeaker_testing_correct,(len(emodb_test_loader)*TEST_BATCH_SIZE),speaker_testing_accuracy*100)) \n",
    "\n",
    "        lfold_emotion_training_accuracies.append(emotion_training_accuracy)\n",
    "        lfold_emotion_validating_accuracies.append(emotion_validating_accuracy)\n",
    "        lfold_emotion_testing_accuracies.append(emotion_testing_accuracy)\n",
    "        lfold_speaker_training_accuracies.append(speaker_training_accuracy)\n",
    "        lfold_speaker_validating_accuracies.append(speaker_validating_accuracy)\n",
    "        lfold_speaker_testing_accuracies.append(speaker_testing_accuracy)\n",
    "\n",
    "print('\\nSUMMARY:')\n",
    "print('\\nCV Emotion Training accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_training_accuracies), stdev(lfold_emotion_training_accuracies)))\n",
    "print('\\nCV Emotion Validating accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_validating_accuracies), stdev(lfold_emotion_validating_accuracies)))\n",
    "print('\\nCV Emotion Testing accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_testing_accuracies), stdev(lfold_emotion_testing_accuracies)))\n",
    "\n",
    "print('\\nCV Speaker Training accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_speaker_training_accuracies), stdev(lfold_speaker_training_accuracies)))\n",
    "print('\\nCV Speaker Validating accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_speaker_validating_accuracies), stdev(lfold_speaker_validating_accuracies)))\n",
    "print('\\nCV Speaker Testing accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_speaker_testing_accuracies), stdev(lfold_speaker_testing_accuracies)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2986.319654,
   "end_time": "2021-02-20T09:54:41.091325",
   "environment_variables": {},
   "exception": null,
   "input_path": "[DANN] EmoDB_MFCC_2L-CNN-GRU_32 training batch size-DAP1.25.ipynb",
   "output_path": "[DANN] EmoDB_MFCC_2L-CNN-GRU_32 training batch size-DAP1.25.ipynb",
   "parameters": {},
   "start_time": "2021-02-20T09:04:54.771671",
   "version": "2.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
