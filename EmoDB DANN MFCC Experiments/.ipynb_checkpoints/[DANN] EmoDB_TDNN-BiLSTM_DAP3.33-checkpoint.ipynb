{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "binding-crime",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:56.082859Z",
     "iopub.status.busy": "2021-02-20T09:04:56.082365Z",
     "iopub.status.idle": "2021-02-20T09:04:58.616402Z",
     "shell.execute_reply": "2021-02-20T09:04:58.616795Z"
    },
    "papermill": {
     "duration": 2.556128,
     "end_time": "2021-02-20T09:04:58.616963",
     "exception": false,
     "start_time": "2021-02-20T09:04:56.060835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan/miniconda3/envs/FYP/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import scipy\n",
    "import math\n",
    "from scipy import signal\n",
    "import librosa.display\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "GAMMA = 2.5\n",
    "DATA = 'emodb_MFCC'\n",
    "ROOT = './Dataset/{}'.format(DATA)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "MODEL_PATH = './model/{}_DANN_TDNN-BiLSTM_DAP{}_CV'.format(DATA,GAMMA)\n",
    "NPARR_PATH = './array/{}_DANN_TDNN-BiLSTM-GRU_DAP{}_CV accuracies.npz'.format(DATA,GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "consistent-christianity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:58.658400Z",
     "iopub.status.busy": "2021-02-20T09:04:58.657747Z",
     "iopub.status.idle": "2021-02-20T09:04:58.662059Z",
     "shell.execute_reply": "2021-02-20T09:04:58.662516Z"
    },
    "papermill": {
     "duration": 0.02674,
     "end_time": "2021-02-20T09:04:58.662624",
     "exception": false,
     "start_time": "2021-02-20T09:04:58.635884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to:42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(sd):\n",
    "    np.random.seed(sd)\n",
    "    random.seed(sd)\n",
    "    random.Random(sd)\n",
    "    torch.manual_seed(sd)\n",
    "    torch.cuda.manual_seed(sd)\n",
    "    torch.cuda.manual_seed_all(sd)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(\"Seed set to:{}\".format(sd))\n",
    "# also set worker_init_fn=np.random.seed(0),num_workers=0 in dataloader   \n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-connectivity",
   "metadata": {
    "papermill": {
     "duration": 0.017615,
     "end_time": "2021-02-20T09:04:58.697626",
     "exception": false,
     "start_time": "2021-02-20T09:04:58.680011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. CV Dataset for EMODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "strange-asset",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:58.746861Z",
     "iopub.status.busy": "2021-02-20T09:04:58.740075Z",
     "iopub.status.idle": "2021-02-20T09:04:58.748970Z",
     "shell.execute_reply": "2021-02-20T09:04:58.748566Z"
    },
    "papermill": {
     "duration": 0.033531,
     "end_time": "2021-02-20T09:04:58.749057",
     "exception": false,
     "start_time": "2021-02-20T09:04:58.715526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmoDBDataset2(object):\n",
    "    \"\"\"\n",
    "        Create a Dataset for EmoDB. Each item is a tuple of the form:\n",
    "        (feature, sample_rate, emotion, speaker)\n",
    "    \"\"\"\n",
    "    _emotions = { 'W': 0, 'L': 1, 'E': 2, 'A': 3, 'F': 4, 'T': 5, 'N': 6 } \n",
    "    # W = anger, L = boredom, E = disgust, A = anxiety/fear, F = happiness, T = sadness, N = neutral\n",
    "    \n",
    "    _speaker = {'03': 0, '08': 1, '09': 2,'10': 3,'11': 4,'12': 5,'13': 6,'14': 7,'15': 8,'16': 9}\n",
    "\n",
    "    def __init__(self, root, cv_index, split):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Directory containing the features files\n",
    "            split(string): Either train, validate or test set\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.data = []\n",
    "        self.df = pd.DataFrame(self.data, columns=['Speaker', 'Emotion', 'File'])\n",
    "        self.cv = { 0: (['12','13','03','08','10','14'],['15','09'],['11','16']),\n",
    "                    1: (['15','09','03','08','10','14'],['11','16'],['12','13']),\n",
    "                    2: (['15','09','11','16','10','14'],['12','13'],['03','08']),\n",
    "                    3: (['15','09','11','16','12','13'],['03','08'],['10','14']),\n",
    "                    4: (['11','16','12','13','03','08'],['10','14'],['15','09'])\n",
    "                  }\n",
    "\n",
    "        # Iterate through all audio files\n",
    "        for root, dirs, files in os.walk(root):\n",
    "            for file in files:\n",
    "                \n",
    "#                 Every utterance is named according to the same scheme:\n",
    "#                 Positions 1-2: speaker_id; \n",
    "#                 Positions 3-5: code for text; \n",
    "#                 Position 6: emotion; \n",
    "#                 Position 7: if there are more than two versions, these are numbered a, b, c\n",
    "                \n",
    "#                 03 - male, 31 years old\n",
    "#                 08 - female, 34 years\n",
    "#                 09 - female, 21 years\n",
    "#                 10 - male, 32 years\n",
    "#                 11 - male, 26 years\n",
    "#                 12 - male, 30 years\n",
    "#                 13 - female, 32 years\n",
    "#                 14 - female, 35 years\n",
    "#                 15 - male, 25 years\n",
    "#                 16 - female, 31 years\n",
    "                \n",
    "                if split == 'train':\n",
    "                    if file[0:2] in self.cv[cv_index][0]:\n",
    "                        self.data.append([file[0:2], file[5], file])\n",
    "                elif split == 'validate':\n",
    "                    if file[0:2] in self.cv[cv_index][1]:\n",
    "                        self.data.append([file[0:2], file[5], file])\n",
    "                elif split == 'test':\n",
    "                    if file[0:2] in self.cv[cv_index][2]:\n",
    "                        self.data.append([file[0:2], file[5], file])\n",
    "                else:\n",
    "                    print(\"Error: Split can only be train, validate or test!\")\n",
    "                        \n",
    "                #self.data.append([file[0:2], file[5], file])\n",
    "\n",
    "        # Convert data to pandas dataframe\n",
    "        self.df = pd.DataFrame(self.data, columns=['Speaker', 'Emotion', 'File'])\n",
    "\n",
    "        # Map emotion labels to numeric values\n",
    "        self.df['Emotion'] = self.df['Emotion'].map(self._emotions).astype(np.long)\n",
    "        self.df['Speaker'] = self.df['Speaker'].map(self._speaker).astype(np.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        file_name = os.path.join(self.root, self.df.loc[idx, 'File'])\n",
    "        feature = torch.load(file_name)\n",
    "        emotion = self.df.loc[idx, 'Emotion']\n",
    "        speaker = self.df.loc[idx, 'Speaker']\n",
    "        \n",
    "        # return a tuple instead of a dictionary\n",
    "        sample = (feature,emotion,speaker)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convinced-gender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>03a07Wc.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>08b09Fd.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>13a07Tc.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>16a05Ea.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>13b09Fc.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>16b01La.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>03b02Aa.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>16b01Aa.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>16a07Td.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>16a02Ea.pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>329 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Speaker  Emotion        File\n",
       "0          0        0  03a07Wc.pt\n",
       "1          1        4  08b09Fd.pt\n",
       "2          6        5  13a07Tc.pt\n",
       "3          9        2  16a05Ea.pt\n",
       "4          6        4  13b09Fc.pt\n",
       "..       ...      ...         ...\n",
       "324        9        1  16b01La.pt\n",
       "325        0        3  03b02Aa.pt\n",
       "326        9        3  16b01Aa.pt\n",
       "327        9        5  16a07Td.pt\n",
       "328        9        2  16a02Ea.pt\n",
       "\n",
       "[329 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize RavdessDataset\n",
    "emodb_dataset_train = EmoDBDataset2(ROOT,cv_index = 4,split= 'train')\n",
    "emodb_dataset_test = EmoDBDataset2(ROOT,cv_index = 4,split= 'test')\n",
    "emodb_dataset_validate = EmoDBDataset2(ROOT,cv_index = 4, split= 'validate')\n",
    "\n",
    "# To view dataframe, uncomment below: \n",
    "emodb_dataset_train.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "arbitrary-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# TRAIN_BATCH_SIZE = 16\n",
    "# VALIDATE_BATCH_SIZE = len(emodb_dataset_validate)\n",
    "# TEST_BATCH_SIZE = len(emodb_dataset_test)\n",
    "# emodb_train_loader = DataLoader(dataset=emodb_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=2, pin_memory=True)\n",
    "# emodb_validate_loader = DataLoader(dataset=emodb_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=2, pin_memory=True)\n",
    "# emodb_test_loader = DataLoader(dataset=emodb_dataset_test, batch_size= TEST_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "statutory-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(emodb_train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-azerbaijan",
   "metadata": {
    "papermill": {
     "duration": 0.018494,
     "end_time": "2021-02-20T09:04:59.074699",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.056205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Network Architectures and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-fluid",
   "metadata": {
    "papermill": {
     "duration": 0.018207,
     "end_time": "2021-02-20T09:04:59.110880",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.092673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "covered-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TDNN(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "                    self, \n",
    "                    input_dim=23, \n",
    "                    output_dim=512,\n",
    "                    context_size=5,\n",
    "                    stride=1,\n",
    "                    dilation=1,\n",
    "                    batch_norm=True,\n",
    "                    dropout_p=0.5\n",
    "                ):\n",
    "        '''\n",
    "        TDNN as defined by https://www.danielpovey.com/files/2015_interspeech_multisplice.pdf\n",
    "        Affine transformation not applied globally to all frames but smaller windows with local context\n",
    "        batch_norm: True to include batch normalisation after the non linearity\n",
    "        \n",
    "        Context size and dilation determine the frames selected\n",
    "        (although context size is not really defined in the traditional sense)\n",
    "        For example:\n",
    "            context size 5 and dilation 1 is equivalent to [-2,-1,0,1,2]\n",
    "            context size 3 and dilation 2 is equivalent to [-2, 0, 2]\n",
    "            context size 1 and dilation 1 is equivalent to [0]\n",
    "        '''\n",
    "        super(TDNN, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.stride = stride\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dilation = dilation\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_norm = batch_norm\n",
    "      \n",
    "        self.kernel = nn.Linear(input_dim*context_size, output_dim)\n",
    "        self.nonlinearity = nn.ReLU()\n",
    "        if self.batch_norm:\n",
    "            self.bn = nn.BatchNorm1d(output_dim)\n",
    "        if self.dropout_p:\n",
    "            self.drop = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        input: size (batch, seq_len, input_features)\n",
    "        outpu: size (batch, new_seq_len, output_features)\n",
    "        '''\n",
    "        _, _, d = x.shape\n",
    "        assert (d == self.input_dim), 'Input dimension was wrong. Expected ({}), got ({})'.format(self.input_dim, d)\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Unfold input into smaller temporal contexts\n",
    "        x = F.unfold(\n",
    "                        x, \n",
    "                        (self.context_size, self.input_dim), \n",
    "                        stride=(1,self.input_dim), \n",
    "                        dilation=(self.dilation,1)\n",
    "                    )\n",
    "\n",
    "        # N, output_dim*context_size, new_t = x.shape\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.kernel(x)\n",
    "        x = self.nonlinearity(x)\n",
    "        \n",
    "        if self.dropout_p:\n",
    "            x = self.drop(x)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            x = x.transpose(1,2)\n",
    "            x = self.bn(x)\n",
    "            x = x.transpose(1,2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dense-gates",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.155659Z",
     "iopub.status.busy": "2021-02-20T09:04:59.155093Z",
     "iopub.status.idle": "2021-02-20T09:04:59.157363Z",
     "shell.execute_reply": "2021-02-20T09:04:59.156887Z"
    },
    "papermill": {
     "duration": 0.028231,
     "end_time": "2021-02-20T09:04:59.157476",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.129245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "#MFCC.shape(1,20,281)\n",
    "class FeatureExtractor(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.TDNN1 = TDNN(input_dim= 20, output_dim=128, context_size=5, dilation=2)\n",
    "        self.TDNN2 = TDNN(input_dim= 128, output_dim=64, context_size=3, dilation=4)\n",
    "        self.LSTM = nn.LSTM(input_size= 64, hidden_size = 64, bidirectional = True, batch_first = True)\n",
    "        self.global_average_pooling = nn.AdaptiveAvgPool2d((8,8))\n",
    "        self.FC = nn.Linear(in_features = 8 * 8, out_features = 512) \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.squeeze(x).permute(0,2,1)\n",
    "        # 1 x 1 is the height and width of each of the 256 output channels\n",
    "        # x = self.feature_extractor(x).squeeze() # can just write .reshape(-1,)\n",
    "        x = self.TDNN1(x)\n",
    "        # print(\"TDNN 1 shape: {}\".format(x.shape)) #batch, new sequence length, output features = 32 x 273 x 128\n",
    "        x = self.TDNN2(x)\n",
    "        # print(\"TDNN 2 shape: {}\".format(x.shape)) #batch, new sequence length, output features = 32 x 265 x 64\n",
    "        output, (hn,cn) = self.LSTM(x)\n",
    "        # print(\"LSTM shape: {}\".format(output.shape)) # 32 x 703 x 128\n",
    "        x = self.global_average_pooling(output)\n",
    "        # print(\"Shape after global average pooling:{}\".format(x.shape))\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        x = self.FC(x)\n",
    "        # print(\"FC shape: {}\".format(x.shape))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fixed-damage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.199330Z",
     "iopub.status.busy": "2021-02-20T09:04:59.198818Z",
     "iopub.status.idle": "2021-02-20T09:04:59.200531Z",
     "shell.execute_reply": "2021-02-20T09:04:59.200903Z"
    },
    "papermill": {
     "duration": 0.025055,
     "end_time": "2021-02-20T09:04:59.201015",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.175960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionClassifier,self).__init__()\n",
    "        self.label_classifier = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(512, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 7),\n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.label_classifier(x)\n",
    "        return F.softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "intelligent-outside",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.242419Z",
     "iopub.status.busy": "2021-02-20T09:04:59.241791Z",
     "iopub.status.idle": "2021-02-20T09:04:59.243665Z",
     "shell.execute_reply": "2021-02-20T09:04:59.244054Z"
    },
    "papermill": {
     "duration": 0.024767,
     "end_time": "2021-02-20T09:04:59.244181",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.219414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpeakerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpeakerClassifier,self).__init__()\n",
    "        self.label_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.label_classifier(x)\n",
    "        return F.softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-legislature",
   "metadata": {
    "papermill": {
     "duration": 0.018735,
     "end_time": "2021-02-20T09:04:59.280907",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.262172",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ultimate-coalition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.320584Z",
     "iopub.status.busy": "2021-02-20T09:04:59.320094Z",
     "iopub.status.idle": "2021-02-20T09:04:59.322355Z",
     "shell.execute_reply": "2021-02-20T09:04:59.321901Z"
    },
    "papermill": {
     "duration": 0.023185,
     "end_time": "2021-02-20T09:04:59.322452",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.299267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def domain_adaptation_parameter(p):\n",
    "    lambda_p = 2. / (1. + np.exp(-GAMMA*p)) - 1\n",
    "    return lambda_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "separate-preparation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.404720Z",
     "iopub.status.busy": "2021-02-20T09:04:59.404219Z",
     "iopub.status.idle": "2021-02-20T09:04:59.406382Z",
     "shell.execute_reply": "2021-02-20T09:04:59.405957Z"
    },
    "papermill": {
     "duration": 0.023878,
     "end_time": "2021-02-20T09:04:59.406476",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.382598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_parameters(PATH):\n",
    "    torch.save({\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'emotion_classifier_state_dict': emotion_classifier.state_dict(),\n",
    "                'speaker_classifier_state_dict': speaker_classifier.state_dict(),\n",
    "                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                'emotion_optimizer_state_dict': emotion_optimizer.state_dict(),\n",
    "                'speaker_optimizer_state_dict': speaker_optimizer.state_dict(),\n",
    "                }, PATH)\n",
    "    print(\"Models' parameters and optimisers' parameters saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "amended-warrant",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.445473Z",
     "iopub.status.busy": "2021-02-20T09:04:59.444951Z",
     "iopub.status.idle": "2021-02-20T09:04:59.446848Z",
     "shell.execute_reply": "2021-02-20T09:04:59.447204Z"
    },
    "papermill": {
     "duration": 0.023252,
     "end_time": "2021-02-20T09:04:59.447333",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.424081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_accuracies_and_losses(PATH):\n",
    "    np.savez(PATH, \n",
    "         emoClassLoss = fold_emotion_class_losses,\n",
    "         emoTrain_acc = fold_emotion_training_accuracies, \n",
    "         emoValidate_acc = fold_emotion_validating_accuracies,\n",
    "         spkClassLoss = fold_speaker_class_losses,\n",
    "         spkTrain_acc = fold_speaker_training_accuracies,\n",
    "         spkValidate_acc = fold_speaker_validating_accuracies,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-roads",
   "metadata": {
    "papermill": {
     "duration": 0.018784,
     "end_time": "2021-02-20T09:04:59.484034",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.465250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-elephant",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:04:59.545284Z",
     "iopub.status.busy": "2021-02-20T09:04:59.538015Z",
     "iopub.status.idle": "2021-02-20T09:54:22.812226Z",
     "shell.execute_reply": "2021-02-20T09:54:22.812679Z"
    },
    "papermill": {
     "duration": 2963.31054,
     "end_time": "2021-02-20T09:54:22.812897",
     "exception": false,
     "start_time": "2021-02-20T09:04:59.502357",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current Fold: 0 | Epoch: 0\n",
      "Emotion Training Correct: 42/288 \n",
      "Emotion Training Accuracy: 14.58333%\n",
      "Speaker Training Correct: 59/288 \n",
      "Speaker Training Accuracy: 20.48611%\n",
      "\n",
      "Emotion Validating Correct: 20/99 \n",
      "Emotion Validating Accuracy: 20.20202%\n",
      "Speaker Validating Correct: 0/99 \n",
      "Speaker Validating Accuracy: 0.00000%\n",
      "\n",
      "Current Fold: 0 | Epoch: 1\n",
      "Emotion Training Correct: 46/288 \n",
      "Emotion Training Accuracy: 15.97222%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 20/99 \n",
      "Emotion Validating Accuracy: 20.20202%\n",
      "Speaker Validating Correct: 0/99 \n",
      "Speaker Validating Accuracy: 0.00000%\n",
      "Models' parameters and optimisers' parameters saved.\n",
      "\n",
      "Current Fold: 0 | Epoch: 2\n",
      "Emotion Training Correct: 125/288 \n",
      "Emotion Training Accuracy: 43.40278%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 49/99 \n",
      "Emotion Validating Accuracy: 49.49495%\n",
      "Speaker Validating Correct: 0/99 \n",
      "Speaker Validating Accuracy: 0.00000%\n",
      "Models' parameters and optimisers' parameters saved.\n",
      "\n",
      "Current Fold: 0 | Epoch: 3\n",
      "Emotion Training Correct: 137/288 \n",
      "Emotion Training Accuracy: 47.56944%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 49/99 \n",
      "Emotion Validating Accuracy: 49.49495%\n",
      "Speaker Validating Correct: 1/99 \n",
      "Speaker Validating Accuracy: 1.01010%\n",
      "Models' parameters and optimisers' parameters saved.\n",
      "\n",
      "Current Fold: 0 | Epoch: 4\n",
      "Emotion Training Correct: 133/288 \n",
      "Emotion Training Accuracy: 46.18056%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 43/99 \n",
      "Emotion Validating Accuracy: 43.43434%\n",
      "Speaker Validating Correct: 15/99 \n",
      "Speaker Validating Accuracy: 15.15152%\n",
      "\n",
      "Current Fold: 0 | Epoch: 5\n",
      "Emotion Training Correct: 160/288 \n",
      "Emotion Training Accuracy: 55.55556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 53/99 \n",
      "Emotion Validating Accuracy: 53.53535%\n",
      "Speaker Validating Correct: 30/99 \n",
      "Speaker Validating Accuracy: 30.30303%\n",
      "Models' parameters and optimisers' parameters saved.\n",
      "\n",
      "Current Fold: 0 | Epoch: 6\n",
      "Emotion Training Correct: 184/288 \n",
      "Emotion Training Accuracy: 63.88889%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 54/99 \n",
      "Emotion Validating Accuracy: 54.54545%\n",
      "Speaker Validating Correct: 34/99 \n",
      "Speaker Validating Accuracy: 34.34343%\n",
      "Models' parameters and optimisers' parameters saved.\n",
      "\n",
      "Current Fold: 0 | Epoch: 7\n",
      "Emotion Training Correct: 176/288 \n",
      "Emotion Training Accuracy: 61.11111%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 52/99 \n",
      "Emotion Validating Accuracy: 52.52525%\n",
      "Speaker Validating Correct: 42/99 \n",
      "Speaker Validating Accuracy: 42.42424%\n",
      "\n",
      "Current Fold: 0 | Epoch: 8\n",
      "Emotion Training Correct: 182/288 \n",
      "Emotion Training Accuracy: 63.19444%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 49/99 \n",
      "Emotion Validating Accuracy: 49.49495%\n",
      "Speaker Validating Correct: 35/99 \n",
      "Speaker Validating Accuracy: 35.35354%\n",
      "\n",
      "Current Fold: 0 | Epoch: 9\n",
      "Emotion Training Correct: 183/288 \n",
      "Emotion Training Accuracy: 63.54167%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 42/99 \n",
      "Emotion Validating Accuracy: 42.42424%\n",
      "Speaker Validating Correct: 35/99 \n",
      "Speaker Validating Accuracy: 35.35354%\n",
      "\n",
      "Current Fold: 0 | Epoch: 10\n",
      "Emotion Training Correct: 194/288 \n",
      "Emotion Training Accuracy: 67.36111%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 45/99 \n",
      "Emotion Validating Accuracy: 45.45455%\n",
      "Speaker Validating Correct: 41/99 \n",
      "Speaker Validating Accuracy: 41.41414%\n",
      "\n",
      "Current Fold: 0 | Epoch: 11\n",
      "Emotion Training Correct: 193/288 \n",
      "Emotion Training Accuracy: 67.01389%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 44/99 \n",
      "Emotion Validating Accuracy: 44.44444%\n",
      "Speaker Validating Correct: 42/99 \n",
      "Speaker Validating Accuracy: 42.42424%\n",
      "\n",
      "Current Fold: 0 | Epoch: 12\n",
      "Emotion Training Correct: 189/288 \n",
      "Emotion Training Accuracy: 65.62500%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 52/99 \n",
      "Emotion Validating Accuracy: 52.52525%\n",
      "Speaker Validating Correct: 34/99 \n",
      "Speaker Validating Accuracy: 34.34343%\n",
      "\n",
      "Current Fold: 0 | Epoch: 13\n",
      "Emotion Training Correct: 183/288 \n",
      "Emotion Training Accuracy: 63.54167%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 53/99 \n",
      "Emotion Validating Accuracy: 53.53535%\n",
      "Speaker Validating Correct: 32/99 \n",
      "Speaker Validating Accuracy: 32.32323%\n",
      "\n",
      "Current Fold: 0 | Epoch: 14\n",
      "Emotion Training Correct: 191/288 \n",
      "Emotion Training Accuracy: 66.31944%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 44/99 \n",
      "Emotion Validating Accuracy: 44.44444%\n",
      "Speaker Validating Correct: 42/99 \n",
      "Speaker Validating Accuracy: 42.42424%\n",
      "\n",
      "Current Fold: 0 | Epoch: 15\n",
      "Emotion Training Correct: 199/288 \n",
      "Emotion Training Accuracy: 69.09722%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 45/99 \n",
      "Emotion Validating Accuracy: 45.45455%\n",
      "Speaker Validating Correct: 36/99 \n",
      "Speaker Validating Accuracy: 36.36364%\n",
      "\n",
      "Current Fold: 0 | Epoch: 16\n",
      "Emotion Training Correct: 196/288 \n",
      "Emotion Training Accuracy: 68.05556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 47/99 \n",
      "Emotion Validating Accuracy: 47.47475%\n",
      "Speaker Validating Correct: 32/99 \n",
      "Speaker Validating Accuracy: 32.32323%\n",
      "\n",
      "Current Fold: 0 | Epoch: 17\n",
      "Emotion Training Correct: 200/288 \n",
      "Emotion Training Accuracy: 69.44444%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 48/99 \n",
      "Emotion Validating Accuracy: 48.48485%\n",
      "Speaker Validating Correct: 38/99 \n",
      "Speaker Validating Accuracy: 38.38384%\n",
      "\n",
      "Current Fold: 0 | Epoch: 18\n",
      "Emotion Training Correct: 191/288 \n",
      "Emotion Training Accuracy: 66.31944%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 51/99 \n",
      "Emotion Validating Accuracy: 51.51515%\n",
      "Speaker Validating Correct: 28/99 \n",
      "Speaker Validating Accuracy: 28.28283%\n",
      "\n",
      "Current Fold: 0 | Epoch: 19\n",
      "Emotion Training Correct: 204/288 \n",
      "Emotion Training Accuracy: 70.83333%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 46/99 \n",
      "Emotion Validating Accuracy: 46.46465%\n",
      "Speaker Validating Correct: 30/99 \n",
      "Speaker Validating Accuracy: 30.30303%\n",
      "\n",
      "Current Fold: 0 | Epoch: 20\n",
      "Emotion Training Correct: 206/288 \n",
      "Emotion Training Accuracy: 71.52778%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 49/99 \n",
      "Emotion Validating Accuracy: 49.49495%\n",
      "Speaker Validating Correct: 35/99 \n",
      "Speaker Validating Accuracy: 35.35354%\n",
      "\n",
      "Current Fold: 0 | Epoch: 21\n",
      "Emotion Training Correct: 196/288 \n",
      "Emotion Training Accuracy: 68.05556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 48/99 \n",
      "Emotion Validating Accuracy: 48.48485%\n",
      "Speaker Validating Correct: 35/99 \n",
      "Speaker Validating Accuracy: 35.35354%\n",
      "\n",
      "Current Fold: 0 | Epoch: 22\n",
      "Emotion Training Correct: 197/288 \n",
      "Emotion Training Accuracy: 68.40278%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 39/99 \n",
      "Speaker Validating Accuracy: 39.39394%\n",
      "\n",
      "Current Fold: 0 | Epoch: 23\n",
      "Emotion Training Correct: 208/288 \n",
      "Emotion Training Accuracy: 72.22222%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 48/99 \n",
      "Emotion Validating Accuracy: 48.48485%\n",
      "Speaker Validating Correct: 38/99 \n",
      "Speaker Validating Accuracy: 38.38384%\n",
      "\n",
      "Current Fold: 0 | Epoch: 24\n",
      "Emotion Training Correct: 208/288 \n",
      "Emotion Training Accuracy: 72.22222%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 47/99 \n",
      "Emotion Validating Accuracy: 47.47475%\n",
      "Speaker Validating Correct: 48/99 \n",
      "Speaker Validating Accuracy: 48.48485%\n",
      "\n",
      "Current Fold: 0 | Epoch: 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Training Correct: 183/288 \n",
      "Emotion Training Accuracy: 63.54167%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 40/99 \n",
      "Speaker Validating Accuracy: 40.40404%\n",
      "\n",
      "Current Fold: 0 | Epoch: 26\n",
      "Emotion Training Correct: 212/288 \n",
      "Emotion Training Accuracy: 73.61111%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 40/99 \n",
      "Speaker Validating Accuracy: 40.40404%\n",
      "\n",
      "Current Fold: 0 | Epoch: 27\n",
      "Emotion Training Correct: 214/288 \n",
      "Emotion Training Accuracy: 74.30556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 51/99 \n",
      "Emotion Validating Accuracy: 51.51515%\n",
      "Speaker Validating Correct: 45/99 \n",
      "Speaker Validating Accuracy: 45.45455%\n",
      "\n",
      "Current Fold: 0 | Epoch: 28\n",
      "Emotion Training Correct: 207/288 \n",
      "Emotion Training Accuracy: 71.87500%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 57/99 \n",
      "Emotion Validating Accuracy: 57.57576%\n",
      "Speaker Validating Correct: 34/99 \n",
      "Speaker Validating Accuracy: 34.34343%\n",
      "Models' parameters and optimisers' parameters saved.\n",
      "\n",
      "Current Fold: 0 | Epoch: 29\n",
      "Emotion Training Correct: 212/288 \n",
      "Emotion Training Accuracy: 73.61111%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 51/99 \n",
      "Emotion Validating Accuracy: 51.51515%\n",
      "Speaker Validating Correct: 45/99 \n",
      "Speaker Validating Accuracy: 45.45455%\n",
      "\n",
      "Current Fold: 0 | Epoch: 30\n",
      "Emotion Training Correct: 221/288 \n",
      "Emotion Training Accuracy: 76.73611%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 53/99 \n",
      "Emotion Validating Accuracy: 53.53535%\n",
      "Speaker Validating Correct: 42/99 \n",
      "Speaker Validating Accuracy: 42.42424%\n",
      "\n",
      "Current Fold: 0 | Epoch: 31\n",
      "Emotion Training Correct: 227/288 \n",
      "Emotion Training Accuracy: 78.81944%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 52/99 \n",
      "Emotion Validating Accuracy: 52.52525%\n",
      "Speaker Validating Correct: 45/99 \n",
      "Speaker Validating Accuracy: 45.45455%\n",
      "\n",
      "Current Fold: 0 | Epoch: 32\n",
      "Emotion Training Correct: 230/288 \n",
      "Emotion Training Accuracy: 79.86111%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 49/99 \n",
      "Emotion Validating Accuracy: 49.49495%\n",
      "Speaker Validating Correct: 46/99 \n",
      "Speaker Validating Accuracy: 46.46465%\n",
      "\n",
      "Current Fold: 0 | Epoch: 33\n",
      "Emotion Training Correct: 231/288 \n",
      "Emotion Training Accuracy: 80.20833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 57/99 \n",
      "Emotion Validating Accuracy: 57.57576%\n",
      "Speaker Validating Correct: 40/99 \n",
      "Speaker Validating Accuracy: 40.40404%\n",
      "Models' parameters and optimisers' parameters saved.\n",
      "\n",
      "Current Fold: 0 | Epoch: 34\n",
      "Emotion Training Correct: 235/288 \n",
      "Emotion Training Accuracy: 81.59722%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 58/99 \n",
      "Emotion Validating Accuracy: 58.58586%\n",
      "Speaker Validating Correct: 35/99 \n",
      "Speaker Validating Accuracy: 35.35354%\n",
      "Models' parameters and optimisers' parameters saved.\n",
      "\n",
      "Current Fold: 0 | Epoch: 35\n",
      "Emotion Training Correct: 244/288 \n",
      "Emotion Training Accuracy: 84.72222%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 52/99 \n",
      "Emotion Validating Accuracy: 52.52525%\n",
      "Speaker Validating Correct: 40/99 \n",
      "Speaker Validating Accuracy: 40.40404%\n",
      "\n",
      "Current Fold: 0 | Epoch: 36\n",
      "Emotion Training Correct: 235/288 \n",
      "Emotion Training Accuracy: 81.59722%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 57/99 \n",
      "Emotion Validating Accuracy: 57.57576%\n",
      "Speaker Validating Correct: 35/99 \n",
      "Speaker Validating Accuracy: 35.35354%\n",
      "\n",
      "Current Fold: 0 | Epoch: 37\n",
      "Emotion Training Correct: 250/288 \n",
      "Emotion Training Accuracy: 86.80556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 56/99 \n",
      "Emotion Validating Accuracy: 56.56566%\n",
      "Speaker Validating Correct: 33/99 \n",
      "Speaker Validating Accuracy: 33.33333%\n",
      "\n",
      "Current Fold: 0 | Epoch: 38\n",
      "Emotion Training Correct: 250/288 \n",
      "Emotion Training Accuracy: 86.80556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 59/99 \n",
      "Emotion Validating Accuracy: 59.59596%\n",
      "Speaker Validating Correct: 35/99 \n",
      "Speaker Validating Accuracy: 35.35354%\n",
      "Models' parameters and optimisers' parameters saved.\n",
      "\n",
      "Current Fold: 0 | Epoch: 39\n",
      "Emotion Training Correct: 246/288 \n",
      "Emotion Training Accuracy: 85.41667%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 52/99 \n",
      "Emotion Validating Accuracy: 52.52525%\n",
      "Speaker Validating Correct: 51/99 \n",
      "Speaker Validating Accuracy: 51.51515%\n",
      "\n",
      "Current Fold: 0 | Epoch: 40\n",
      "Emotion Training Correct: 238/288 \n",
      "Emotion Training Accuracy: 82.63889%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 55/99 \n",
      "Emotion Validating Accuracy: 55.55556%\n",
      "Speaker Validating Correct: 29/99 \n",
      "Speaker Validating Accuracy: 29.29293%\n",
      "\n",
      "Current Fold: 0 | Epoch: 41\n",
      "Emotion Training Correct: 250/288 \n",
      "Emotion Training Accuracy: 86.80556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 61/99 \n",
      "Emotion Validating Accuracy: 61.61616%\n",
      "Speaker Validating Correct: 31/99 \n",
      "Speaker Validating Accuracy: 31.31313%\n",
      "Models' parameters and optimisers' parameters saved.\n",
      "\n",
      "Current Fold: 0 | Epoch: 42\n",
      "Emotion Training Correct: 254/288 \n",
      "Emotion Training Accuracy: 88.19444%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 44/99 \n",
      "Emotion Validating Accuracy: 44.44444%\n",
      "Speaker Validating Correct: 50/99 \n",
      "Speaker Validating Accuracy: 50.50505%\n",
      "\n",
      "Current Fold: 0 | Epoch: 43\n",
      "Emotion Training Correct: 260/288 \n",
      "Emotion Training Accuracy: 90.27778%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 55/99 \n",
      "Emotion Validating Accuracy: 55.55556%\n",
      "Speaker Validating Correct: 29/99 \n",
      "Speaker Validating Accuracy: 29.29293%\n",
      "\n",
      "Current Fold: 0 | Epoch: 44\n",
      "Emotion Training Correct: 256/288 \n",
      "Emotion Training Accuracy: 88.88889%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 54/99 \n",
      "Emotion Validating Accuracy: 54.54545%\n",
      "Speaker Validating Correct: 31/99 \n",
      "Speaker Validating Accuracy: 31.31313%\n",
      "\n",
      "Current Fold: 0 | Epoch: 45\n",
      "Emotion Training Correct: 251/288 \n",
      "Emotion Training Accuracy: 87.15278%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 56/99 \n",
      "Emotion Validating Accuracy: 56.56566%\n",
      "Speaker Validating Correct: 42/99 \n",
      "Speaker Validating Accuracy: 42.42424%\n",
      "\n",
      "Current Fold: 0 | Epoch: 46\n",
      "Emotion Training Correct: 244/288 \n",
      "Emotion Training Accuracy: 84.72222%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 61/99 \n",
      "Emotion Validating Accuracy: 61.61616%\n",
      "Speaker Validating Correct: 38/99 \n",
      "Speaker Validating Accuracy: 38.38384%\n",
      "Models' parameters and optimisers' parameters saved.\n",
      "\n",
      "Current Fold: 0 | Epoch: 47\n",
      "Emotion Training Correct: 261/288 \n",
      "Emotion Training Accuracy: 90.62500%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 56/99 \n",
      "Emotion Validating Accuracy: 56.56566%\n",
      "Speaker Validating Correct: 38/99 \n",
      "Speaker Validating Accuracy: 38.38384%\n",
      "\n",
      "Current Fold: 0 | Epoch: 48\n",
      "Emotion Training Correct: 268/288 \n",
      "Emotion Training Accuracy: 93.05556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 55/99 \n",
      "Emotion Validating Accuracy: 55.55556%\n",
      "Speaker Validating Correct: 37/99 \n",
      "Speaker Validating Accuracy: 37.37374%\n",
      "\n",
      "Current Fold: 0 | Epoch: 49\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 59/99 \n",
      "Emotion Validating Accuracy: 59.59596%\n",
      "Speaker Validating Correct: 40/99 \n",
      "Speaker Validating Accuracy: 40.40404%\n",
      "\n",
      "Current Fold: 0 | Epoch: 50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Training Correct: 263/288 \n",
      "Emotion Training Accuracy: 91.31944%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 56/99 \n",
      "Emotion Validating Accuracy: 56.56566%\n",
      "Speaker Validating Correct: 40/99 \n",
      "Speaker Validating Accuracy: 40.40404%\n",
      "\n",
      "Current Fold: 0 | Epoch: 51\n",
      "Emotion Training Correct: 263/288 \n",
      "Emotion Training Accuracy: 91.31944%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 58/99 \n",
      "Emotion Validating Accuracy: 58.58586%\n",
      "Speaker Validating Correct: 39/99 \n",
      "Speaker Validating Accuracy: 39.39394%\n",
      "\n",
      "Current Fold: 0 | Epoch: 52\n",
      "Emotion Training Correct: 266/288 \n",
      "Emotion Training Accuracy: 92.36111%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 47/99 \n",
      "Emotion Validating Accuracy: 47.47475%\n",
      "Speaker Validating Correct: 42/99 \n",
      "Speaker Validating Accuracy: 42.42424%\n",
      "\n",
      "Current Fold: 0 | Epoch: 53\n",
      "Emotion Training Correct: 263/288 \n",
      "Emotion Training Accuracy: 91.31944%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 54/99 \n",
      "Emotion Validating Accuracy: 54.54545%\n",
      "Speaker Validating Correct: 41/99 \n",
      "Speaker Validating Accuracy: 41.41414%\n",
      "\n",
      "Current Fold: 0 | Epoch: 54\n",
      "Emotion Training Correct: 255/288 \n",
      "Emotion Training Accuracy: 88.54167%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 55/99 \n",
      "Emotion Validating Accuracy: 55.55556%\n",
      "Speaker Validating Correct: 43/99 \n",
      "Speaker Validating Accuracy: 43.43434%\n",
      "\n",
      "Current Fold: 0 | Epoch: 55\n",
      "Emotion Training Correct: 264/288 \n",
      "Emotion Training Accuracy: 91.66667%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 51/99 \n",
      "Emotion Validating Accuracy: 51.51515%\n",
      "Speaker Validating Correct: 37/99 \n",
      "Speaker Validating Accuracy: 37.37374%\n",
      "\n",
      "Current Fold: 0 | Epoch: 56\n",
      "Emotion Training Correct: 261/288 \n",
      "Emotion Training Accuracy: 90.62500%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 58/99 \n",
      "Emotion Validating Accuracy: 58.58586%\n",
      "Speaker Validating Correct: 22/99 \n",
      "Speaker Validating Accuracy: 22.22222%\n",
      "\n",
      "Current Fold: 0 | Epoch: 57\n",
      "Emotion Training Correct: 262/288 \n",
      "Emotion Training Accuracy: 90.97222%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 55/99 \n",
      "Emotion Validating Accuracy: 55.55556%\n",
      "Speaker Validating Correct: 27/99 \n",
      "Speaker Validating Accuracy: 27.27273%\n",
      "\n",
      "Current Fold: 0 | Epoch: 58\n",
      "Emotion Training Correct: 266/288 \n",
      "Emotion Training Accuracy: 92.36111%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 49/99 \n",
      "Emotion Validating Accuracy: 49.49495%\n",
      "Speaker Validating Correct: 49/99 \n",
      "Speaker Validating Accuracy: 49.49495%\n",
      "\n",
      "Current Fold: 0 | Epoch: 59\n",
      "Emotion Training Correct: 264/288 \n",
      "Emotion Training Accuracy: 91.66667%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 33/99 \n",
      "Speaker Validating Accuracy: 33.33333%\n",
      "\n",
      "Current Fold: 0 | Epoch: 60\n",
      "Emotion Training Correct: 263/288 \n",
      "Emotion Training Accuracy: 91.31944%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 60/99 \n",
      "Emotion Validating Accuracy: 60.60606%\n",
      "Speaker Validating Correct: 28/99 \n",
      "Speaker Validating Accuracy: 28.28283%\n",
      "\n",
      "Current Fold: 0 | Epoch: 61\n",
      "Emotion Training Correct: 264/288 \n",
      "Emotion Training Accuracy: 91.66667%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 53/99 \n",
      "Emotion Validating Accuracy: 53.53535%\n",
      "Speaker Validating Correct: 36/99 \n",
      "Speaker Validating Accuracy: 36.36364%\n",
      "\n",
      "Current Fold: 0 | Epoch: 62\n",
      "Emotion Training Correct: 266/288 \n",
      "Emotion Training Accuracy: 92.36111%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 48/99 \n",
      "Emotion Validating Accuracy: 48.48485%\n",
      "Speaker Validating Correct: 39/99 \n",
      "Speaker Validating Accuracy: 39.39394%\n",
      "\n",
      "Current Fold: 0 | Epoch: 63\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 60/99 \n",
      "Emotion Validating Accuracy: 60.60606%\n",
      "Speaker Validating Correct: 34/99 \n",
      "Speaker Validating Accuracy: 34.34343%\n",
      "\n",
      "Current Fold: 0 | Epoch: 64\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 57/99 \n",
      "Emotion Validating Accuracy: 57.57576%\n",
      "Speaker Validating Correct: 36/99 \n",
      "Speaker Validating Accuracy: 36.36364%\n",
      "\n",
      "Current Fold: 0 | Epoch: 65\n",
      "Emotion Training Correct: 251/288 \n",
      "Emotion Training Accuracy: 87.15278%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 57/99 \n",
      "Emotion Validating Accuracy: 57.57576%\n",
      "Speaker Validating Correct: 51/99 \n",
      "Speaker Validating Accuracy: 51.51515%\n",
      "\n",
      "Current Fold: 0 | Epoch: 66\n",
      "Emotion Training Correct: 265/288 \n",
      "Emotion Training Accuracy: 92.01389%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 51/99 \n",
      "Emotion Validating Accuracy: 51.51515%\n",
      "Speaker Validating Correct: 51/99 \n",
      "Speaker Validating Accuracy: 51.51515%\n",
      "\n",
      "Current Fold: 0 | Epoch: 67\n",
      "Emotion Training Correct: 268/288 \n",
      "Emotion Training Accuracy: 93.05556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 51/99 \n",
      "Speaker Validating Accuracy: 51.51515%\n",
      "\n",
      "Current Fold: 0 | Epoch: 68\n",
      "Emotion Training Correct: 264/288 \n",
      "Emotion Training Accuracy: 91.66667%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 58/99 \n",
      "Emotion Validating Accuracy: 58.58586%\n",
      "Speaker Validating Correct: 38/99 \n",
      "Speaker Validating Accuracy: 38.38384%\n",
      "\n",
      "Current Fold: 0 | Epoch: 69\n",
      "Emotion Training Correct: 263/288 \n",
      "Emotion Training Accuracy: 91.31944%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 54/99 \n",
      "Emotion Validating Accuracy: 54.54545%\n",
      "Speaker Validating Correct: 48/99 \n",
      "Speaker Validating Accuracy: 48.48485%\n",
      "\n",
      "Current Fold: 0 | Epoch: 70\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 44/99 \n",
      "Emotion Validating Accuracy: 44.44444%\n",
      "Speaker Validating Correct: 51/99 \n",
      "Speaker Validating Accuracy: 51.51515%\n",
      "\n",
      "Current Fold: 0 | Epoch: 71\n",
      "Emotion Training Correct: 264/288 \n",
      "Emotion Training Accuracy: 91.66667%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 52/99 \n",
      "Emotion Validating Accuracy: 52.52525%\n",
      "Speaker Validating Correct: 47/99 \n",
      "Speaker Validating Accuracy: 47.47475%\n",
      "\n",
      "Current Fold: 0 | Epoch: 72\n",
      "Emotion Training Correct: 269/288 \n",
      "Emotion Training Accuracy: 93.40278%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 59/99 \n",
      "Emotion Validating Accuracy: 59.59596%\n",
      "Speaker Validating Correct: 33/99 \n",
      "Speaker Validating Accuracy: 33.33333%\n",
      "\n",
      "Current Fold: 0 | Epoch: 73\n",
      "Emotion Training Correct: 269/288 \n",
      "Emotion Training Accuracy: 93.40278%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 53/99 \n",
      "Emotion Validating Accuracy: 53.53535%\n",
      "Speaker Validating Correct: 37/99 \n",
      "Speaker Validating Accuracy: 37.37374%\n",
      "\n",
      "Current Fold: 0 | Epoch: 74\n",
      "Emotion Training Correct: 268/288 \n",
      "Emotion Training Accuracy: 93.05556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 55/99 \n",
      "Emotion Validating Accuracy: 55.55556%\n",
      "Speaker Validating Correct: 45/99 \n",
      "Speaker Validating Accuracy: 45.45455%\n",
      "\n",
      "Current Fold: 0 | Epoch: 75\n",
      "Emotion Training Correct: 268/288 \n",
      "Emotion Training Accuracy: 93.05556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 39/99 \n",
      "Speaker Validating Accuracy: 39.39394%\n",
      "\n",
      "Current Fold: 0 | Epoch: 76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Training Correct: 266/288 \n",
      "Emotion Training Accuracy: 92.36111%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 56/99 \n",
      "Emotion Validating Accuracy: 56.56566%\n",
      "Speaker Validating Correct: 38/99 \n",
      "Speaker Validating Accuracy: 38.38384%\n",
      "\n",
      "Current Fold: 0 | Epoch: 77\n",
      "Emotion Training Correct: 265/288 \n",
      "Emotion Training Accuracy: 92.01389%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 60/99 \n",
      "Emotion Validating Accuracy: 60.60606%\n",
      "Speaker Validating Correct: 39/99 \n",
      "Speaker Validating Accuracy: 39.39394%\n",
      "\n",
      "Current Fold: 0 | Epoch: 78\n",
      "Emotion Training Correct: 269/288 \n",
      "Emotion Training Accuracy: 93.40278%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 48/99 \n",
      "Emotion Validating Accuracy: 48.48485%\n",
      "Speaker Validating Correct: 47/99 \n",
      "Speaker Validating Accuracy: 47.47475%\n",
      "\n",
      "Current Fold: 0 | Epoch: 79\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 38/99 \n",
      "Speaker Validating Accuracy: 38.38384%\n",
      "\n",
      "Current Fold: 0 | Epoch: 80\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 51/99 \n",
      "Emotion Validating Accuracy: 51.51515%\n",
      "Speaker Validating Correct: 44/99 \n",
      "Speaker Validating Accuracy: 44.44444%\n",
      "\n",
      "Current Fold: 0 | Epoch: 81\n",
      "Emotion Training Correct: 263/288 \n",
      "Emotion Training Accuracy: 91.31944%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 47/99 \n",
      "Speaker Validating Accuracy: 47.47475%\n",
      "\n",
      "Current Fold: 0 | Epoch: 82\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 48/99 \n",
      "Emotion Validating Accuracy: 48.48485%\n",
      "Speaker Validating Correct: 39/99 \n",
      "Speaker Validating Accuracy: 39.39394%\n",
      "\n",
      "Current Fold: 0 | Epoch: 83\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 48/99 \n",
      "Emotion Validating Accuracy: 48.48485%\n",
      "Speaker Validating Correct: 38/99 \n",
      "Speaker Validating Accuracy: 38.38384%\n",
      "\n",
      "Current Fold: 0 | Epoch: 84\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 52/99 \n",
      "Emotion Validating Accuracy: 52.52525%\n",
      "Speaker Validating Correct: 40/99 \n",
      "Speaker Validating Accuracy: 40.40404%\n",
      "\n",
      "Current Fold: 0 | Epoch: 85\n",
      "Emotion Training Correct: 271/288 \n",
      "Emotion Training Accuracy: 94.09722%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 52/99 \n",
      "Emotion Validating Accuracy: 52.52525%\n",
      "Speaker Validating Correct: 42/99 \n",
      "Speaker Validating Accuracy: 42.42424%\n",
      "\n",
      "Current Fold: 0 | Epoch: 86\n",
      "Emotion Training Correct: 264/288 \n",
      "Emotion Training Accuracy: 91.66667%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 57/99 \n",
      "Emotion Validating Accuracy: 57.57576%\n",
      "Speaker Validating Correct: 44/99 \n",
      "Speaker Validating Accuracy: 44.44444%\n",
      "\n",
      "Current Fold: 0 | Epoch: 87\n",
      "Emotion Training Correct: 266/288 \n",
      "Emotion Training Accuracy: 92.36111%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 55/99 \n",
      "Speaker Validating Accuracy: 55.55556%\n",
      "\n",
      "Current Fold: 0 | Epoch: 88\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 54/99 \n",
      "Emotion Validating Accuracy: 54.54545%\n",
      "Speaker Validating Correct: 51/99 \n",
      "Speaker Validating Accuracy: 51.51515%\n",
      "\n",
      "Current Fold: 0 | Epoch: 89\n",
      "Emotion Training Correct: 261/288 \n",
      "Emotion Training Accuracy: 90.62500%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 49/99 \n",
      "Emotion Validating Accuracy: 49.49495%\n",
      "Speaker Validating Correct: 48/99 \n",
      "Speaker Validating Accuracy: 48.48485%\n",
      "\n",
      "Current Fold: 0 | Epoch: 90\n",
      "Emotion Training Correct: 266/288 \n",
      "Emotion Training Accuracy: 92.36111%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 48/99 \n",
      "Speaker Validating Accuracy: 48.48485%\n",
      "\n",
      "Current Fold: 0 | Epoch: 91\n",
      "Emotion Training Correct: 269/288 \n",
      "Emotion Training Accuracy: 93.40278%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 43/99 \n",
      "Speaker Validating Accuracy: 43.43434%\n",
      "\n",
      "Current Fold: 0 | Epoch: 92\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 49/99 \n",
      "Emotion Validating Accuracy: 49.49495%\n",
      "Speaker Validating Correct: 42/99 \n",
      "Speaker Validating Accuracy: 42.42424%\n",
      "\n",
      "Current Fold: 0 | Epoch: 93\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 43/99 \n",
      "Speaker Validating Accuracy: 43.43434%\n",
      "\n",
      "Current Fold: 0 | Epoch: 94\n",
      "Emotion Training Correct: 268/288 \n",
      "Emotion Training Accuracy: 93.05556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 50/99 \n",
      "Emotion Validating Accuracy: 50.50505%\n",
      "Speaker Validating Correct: 47/99 \n",
      "Speaker Validating Accuracy: 47.47475%\n",
      "\n",
      "Current Fold: 0 | Epoch: 95\n",
      "Emotion Training Correct: 268/288 \n",
      "Emotion Training Accuracy: 93.05556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 52/99 \n",
      "Emotion Validating Accuracy: 52.52525%\n",
      "Speaker Validating Correct: 47/99 \n",
      "Speaker Validating Accuracy: 47.47475%\n",
      "\n",
      "Current Fold: 0 | Epoch: 96\n",
      "Emotion Training Correct: 268/288 \n",
      "Emotion Training Accuracy: 93.05556%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 47/99 \n",
      "Emotion Validating Accuracy: 47.47475%\n",
      "Speaker Validating Correct: 48/99 \n",
      "Speaker Validating Accuracy: 48.48485%\n",
      "\n",
      "Current Fold: 0 | Epoch: 97\n",
      "Emotion Training Correct: 270/288 \n",
      "Emotion Training Accuracy: 93.75000%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 55/99 \n",
      "Emotion Validating Accuracy: 55.55556%\n",
      "Speaker Validating Correct: 43/99 \n",
      "Speaker Validating Accuracy: 43.43434%\n",
      "\n",
      "Current Fold: 0 | Epoch: 98\n",
      "Emotion Training Correct: 267/288 \n",
      "Emotion Training Accuracy: 92.70833%\n",
      "Speaker Training Correct: 0/288 \n",
      "Speaker Training Accuracy: 0.00000%\n",
      "\n",
      "Emotion Validating Correct: 53/99 \n",
      "Emotion Validating Accuracy: 53.53535%\n",
      "Speaker Validating Correct: 45/99 \n",
      "Speaker Validating Accuracy: 45.45455%\n",
      "\n",
      "Current Fold: 0 | Epoch: 99\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.optim as optim\n",
    "import pylab as plt\n",
    "\n",
    "# Read this to understand how GRL works: \n",
    "# https://christineai.blog/category/domain-adaptation/\n",
    "\n",
    "FOLDS = 5\n",
    "EPOCHS = 100\n",
    "\n",
    "############### To comment this section out if disrupted #############\n",
    "fold_emotion_class_losses = np.zeros((FOLDS,EPOCHS))\n",
    "fold_emotion_training_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "fold_emotion_validating_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "fold_speaker_class_losses = np.zeros((FOLDS,EPOCHS))\n",
    "fold_speaker_training_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "fold_speaker_validating_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "\n",
    "\n",
    "######### To uncomment if trying to continue disrupted training ######\n",
    "# fold_emotion_class_losses, fold_emotion_training_accuracies, fold_emotion_validating_accuracies， fold_speaker_class_losses，fold_speaker_training_accuracies，fold_speaker_validating_accuracies= np.load(NPARR_PATH)\n",
    "\n",
    "###################################################################\n",
    "\n",
    "# 5-fold cross validation\n",
    "for fold in range(0,FOLDS):\n",
    "\n",
    "    # Selecting CPU or GPU\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    # Selecting the type of encoder, label classifier\n",
    "    encoder = FeatureExtractor().to(DEVICE).train()\n",
    "    emotion_classifier = EmotionClassifier().to(DEVICE).train()\n",
    "    speaker_classifier = SpeakerClassifier().to(DEVICE).train()\n",
    "    \n",
    "#     encoder = nn.DataParallel(encoder)\n",
    "#     emotion_classifier = nn.DataParallel(emotion_classifier)\n",
    "#     speaker_classifier = nn.DataParallel(speaker_classifier)\n",
    "\n",
    "    # Optimizer \n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters())\n",
    "    emotion_optimizer = torch.optim.Adam(emotion_classifier.parameters())\n",
    "    speaker_optimizer = torch.optim.Adam(speaker_classifier.parameters())\n",
    "\n",
    "    cross_entropy_loss = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    \n",
    "    emodb_dataset_train = EmoDBDataset2(ROOT,cv_index = fold ,split= 'train')\n",
    "    emodb_dataset_test = EmoDBDataset2(ROOT,cv_index = fold, split= 'test')\n",
    "    emodb_dataset_validate = EmoDBDataset2(ROOT ,cv_index = fold, split= 'validate')\n",
    "    \n",
    "    # DANN should be trained on labelled data from the source domain and unlabelled data from the target domain\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    emodb_train_loader = DataLoader(dataset=emodb_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=True,worker_init_fn=np.random.seed(42),num_workers=4, pin_memory= True)\n",
    "    \n",
    "    # For evaluation purposes\n",
    "    VALIDATE_BATCH_SIZE = len(emodb_dataset_validate)\n",
    "    emodb_validate_loader = DataLoader(dataset=emodb_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=4, pin_memory= True)\n",
    "    \n",
    "    epoch_emotion_class_losses = []\n",
    "    emotion_training_accuracies = []\n",
    "    emotion_validating_accuracies = []\n",
    "\n",
    "    epoch_speaker_class_losses = []\n",
    "    speaker_training_accuracies = []\n",
    "    speaker_validating_accuracies = []\n",
    "\n",
    "    STEP = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"\\nCurrent Fold: {} | Epoch: {}\".format(fold, epoch))\n",
    "\n",
    "        completed_start_steps = epoch * len(emodb_train_loader)\n",
    "        total_steps = EPOCHS * len(emodb_train_loader)\n",
    "\n",
    "        batch_emotion_class_losses = []\n",
    "        batch_speaker_class_losses = []\n",
    "\n",
    "\n",
    "        for batch_idx, (feature, emotion, speaker) in enumerate(emodb_train_loader):\n",
    "\n",
    "            # Assigned to DEVICE. \n",
    "            features, emotion, speaker = feature.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE)\n",
    "            \n",
    "            # Computing the training progress\n",
    "            p = (batch_idx + completed_start_steps) / total_steps\n",
    "            lambda_p = domain_adaptation_parameter(p)\n",
    "\n",
    "            # Calculate speaker and emotion classification prediction \n",
    "            conv_features = encoder(features)\n",
    "            emotion_preds = emotion_classifier(conv_features)\n",
    "            emotion_class_loss = cross_entropy_loss(emotion_preds, emotion)\n",
    "            speaker_preds = speaker_classifier(conv_features)\n",
    "            speaker_class_loss = cross_entropy_loss(speaker_preds, speaker)\n",
    "\n",
    "            # Calculate total loss\n",
    "            total_loss = emotion_class_loss - lambda_p * speaker_class_loss \n",
    "\n",
    "            # Clear the gradient to prevent gradient accumulation\n",
    "            encoder.zero_grad(set_to_none= True)\n",
    "            emotion_classifier.zero_grad(set_to_none= True)\n",
    "            speaker_classifier.zero_grad(set_to_none= True)\n",
    "\n",
    "            # Computing the gradient\n",
    "            total_loss.backward(retain_graph=True)\n",
    "\n",
    "            # Update the weight\n",
    "            emotion_optimizer.step()\n",
    "            speaker_optimizer.step()\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "            batch_emotion_class_losses.append(emotion_class_loss.detach())\n",
    "            batch_speaker_class_losses.append(speaker_class_loss.detach())\n",
    "\n",
    "\n",
    "        # Enter evaluation mode at the end of each epoch\n",
    "        encoder.eval()\n",
    "        emotion_classifier.eval()\n",
    "        speaker_classifier.eval()\n",
    "\n",
    "        emotion_training_correct, emotion_validating_correct, speaker_training_correct, speaker_validating_correct = 0 , 0 , 0 , 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "        # 1. Training Classification\n",
    "          for index, (features, emotion, speaker) in enumerate(emodb_train_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            emotion_training_correct += (emotion_preds == emotion).sum() \n",
    "            speaker_training_correct += (speaker_preds == speaker).sum() \n",
    "          #source_accuracy = torch.true_divide(source_correct, len(svhn_test_loader.dataset))\n",
    "          emotion_training_accuracy = emotion_training_correct.item()/(len(emodb_train_loader)*TRAIN_BATCH_SIZE)\n",
    "          emotion_training_accuracies.append(emotion_training_accuracy)\n",
    "          speaker_training_accuracy = speaker_training_correct.item()/(len(emodb_train_loader)*TRAIN_BATCH_SIZE)\n",
    "          speaker_training_accuracies.append(speaker_training_accuracy)\n",
    "          print(\"Emotion Training Correct: {}/{} \\nEmotion Training Accuracy: {:.5f}%\".format(emotion_training_correct,(len(emodb_train_loader)*TRAIN_BATCH_SIZE),emotion_training_accuracy*100))\n",
    "          print(\"Speaker Training Correct: {}/{} \\nSpeaker Training Accuracy: {:.5f}%\".format(speaker_training_correct,(len(emodb_train_loader)*TRAIN_BATCH_SIZE),speaker_training_accuracy*100))\n",
    "\n",
    "        # 2. Validating Classification\n",
    "          for index, (features, emotion, speaker) in enumerate(emodb_validate_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            emotion_validating_correct += (emotion_preds == emotion).sum() \n",
    "            speaker_validating_correct += (speaker_preds == speaker).sum() \n",
    "          #source_accuracy = torch.true_divide(source_correct, len(svhn_test_loader.dataset))\n",
    "          emotion_validating_accuracy = emotion_validating_correct.item()/(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "          emotion_validating_accuracies.append(emotion_validating_accuracy)\n",
    "          speaker_validating_accuracy = speaker_validating_correct.item()/(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "          speaker_validating_accuracies.append(speaker_validating_accuracy)\n",
    "          print(\"\\nEmotion Validating Correct: {}/{} \\nEmotion Validating Accuracy: {:.5f}%\".format(emotion_validating_correct,(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE),emotion_validating_accuracy*100))\n",
    "          print(\"Speaker Validating Correct: {}/{} \\nSpeaker Validating Accuracy: {:.5f}%\".format(speaker_validating_correct,(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE),speaker_validating_accuracy*100))\n",
    "\n",
    "          if (len(emotion_validating_accuracies)> 1 and emotion_validating_accuracy >= max(emotion_validating_accuracies[:-1])):\n",
    "                save_parameters(MODEL_PATH + 'fold' + str(fold))\n",
    "\n",
    "\n",
    "        encoder.train()\n",
    "        emotion_classifier.train()\n",
    "        speaker_classifier.train()\n",
    "\n",
    "\n",
    "        epoch_emotion_class_loss = torch.mean(torch.stack(batch_emotion_class_losses), dim=0)\n",
    "        epoch_emotion_class_losses.append(epoch_emotion_class_loss)\n",
    "        epoch_speaker_class_loss = torch.mean(torch.stack(batch_speaker_class_losses), dim=0)\n",
    "        epoch_speaker_class_losses.append(epoch_speaker_class_loss)\n",
    "        \n",
    "    fold_emotion_class_losses[fold] = epoch_emotion_class_losses\n",
    "    fold_emotion_training_accuracies[fold] = emotion_training_accuracies\n",
    "    fold_emotion_validating_accuracies[fold] = emotion_validating_accuracies\n",
    "    fold_speaker_class_losses[fold] = epoch_speaker_class_losses\n",
    "    fold_speaker_training_accuracies[fold] = speaker_training_accuracies\n",
    "    fold_speaker_validating_accuracies[fold] = speaker_validating_accuracies\n",
    "    \n",
    "    save_accuracies_and_losses(NPARR_PATH)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-xerox",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:54:23.326127Z",
     "iopub.status.busy": "2021-02-20T09:54:23.320786Z",
     "iopub.status.idle": "2021-02-20T09:54:23.630414Z",
     "shell.execute_reply": "2021-02-20T09:54:23.630869Z"
    },
    "papermill": {
     "duration": 0.578632,
     "end_time": "2021-02-20T09:54:23.631014",
     "exception": false,
     "start_time": "2021-02-20T09:54:23.052382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Losses vs. epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Losses')\n",
    "\n",
    "for i in range(FOLDS):\n",
    "    plt.plot(range(EPOCHS), fold_emotion_class_losses[i],label='emotion classification loss fold {}'.format(i))\n",
    "    plt.plot(range(EPOCHS), fold_speaker_class_losses[i],label='speaker classification loss fold {}'.format(i))\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-mission",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:54:24.126173Z",
     "iopub.status.busy": "2021-02-20T09:54:24.125448Z",
     "iopub.status.idle": "2021-02-20T09:54:24.126985Z",
     "shell.execute_reply": "2021-02-20T09:54:24.127428Z"
    },
    "papermill": {
     "duration": 0.25619,
     "end_time": "2021-02-20T09:54:24.127572",
     "exception": false,
     "start_time": "2021-02-20T09:54:23.871382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_accuracies_vs_epochs(fold):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.title('Accuracies vs. epochs')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.plot(range(EPOCHS), fold_emotion_training_accuracies[fold],label='emotion_training_accuracies fold {}'.format(fold))\n",
    "    plt.plot(range(EPOCHS), fold_emotion_validating_accuracies[fold],label='emotion_validating_accuracies fold {}'.format(fold))\n",
    "    plt.plot(range(EPOCHS), fold_speaker_training_accuracies[fold],label='speaker_training_accuracies fold {}'.format(fold))\n",
    "    plt.plot(range(EPOCHS), fold_speaker_validating_accuracies[fold],label='speaker_validating_accuracies fold {}'.format(fold))\n",
    "    plt.legend(loc='best')\n",
    "    plt.show() \n",
    "    print(\"Maximum emotion training accuracy:{:.2f}%\".format(max(fold_emotion_training_accuracies[fold])*100))\n",
    "    print(\"Maximum emotion validating accuracy:{:.2f}%\".format(max(fold_emotion_validating_accuracies[fold])*100))\n",
    "    print(\"Maximum speaker training accuracy:{:.2f}%\".format(max(fold_speaker_training_accuracies[fold])*100))\n",
    "    print(\"Maximum speaker validating accuracy:{:.2f}%\".format(max(fold_speaker_validating_accuracies[fold])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-capacity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:54:24.627534Z",
     "iopub.status.busy": "2021-02-20T09:54:24.626950Z",
     "iopub.status.idle": "2021-02-20T09:54:25.636654Z",
     "shell.execute_reply": "2021-02-20T09:54:25.636252Z"
    },
    "papermill": {
     "duration": 1.270501,
     "end_time": "2021-02-20T09:54:25.636773",
     "exception": false,
     "start_time": "2021-02-20T09:54:24.366272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(FOLDS):\n",
    "    plot_accuracies_vs_epochs(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-exploration",
   "metadata": {
    "papermill": {
     "duration": 0.26577,
     "end_time": "2021-02-20T09:54:26.153738",
     "exception": false,
     "start_time": "2021-02-20T09:54:25.887968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Loading and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-involvement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-20T09:54:26.715237Z",
     "iopub.status.busy": "2021-02-20T09:54:26.706591Z",
     "iopub.status.idle": "2021-02-20T09:54:37.603591Z",
     "shell.execute_reply": "2021-02-20T09:54:37.604012Z"
    },
    "papermill": {
     "duration": 11.183106,
     "end_time": "2021-02-20T09:54:37.604178",
     "exception": false,
     "start_time": "2021-02-20T09:54:26.421072",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from statistics import mean , stdev\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "\n",
    "lfold_emotion_training_accuracies, lfold_emotion_validating_accuracies, lfold_emotion_testing_accuracies = [] , [] , []\n",
    "lfold_speaker_training_accuracies, lfold_speaker_validating_accuracies, lfold_speaker_testing_accuracies = [] , [] , []\n",
    "\n",
    "for fold in range(5):\n",
    "    print(\"\\nEvaluation for fold {}\".format(fold))\n",
    "    checkpoint = torch.load(MODEL_PATH + 'fold' + str(fold))\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    encoder = FeatureExtractor().to(DEVICE).train()\n",
    "    emotion_classifier = EmotionClassifier().to(DEVICE).train()\n",
    "    speaker_classifier = SpeakerClassifier().to(DEVICE)\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters())\n",
    "    emotion_optimizer = torch.optim.Adam(emotion_classifier.parameters())\n",
    "    speaker_optimizer = torch.optim.Adam(speaker_classifier.parameters())\n",
    "    \n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    emotion_classifier.load_state_dict(checkpoint['emotion_classifier_state_dict'])\n",
    "    speaker_classifier.load_state_dict(checkpoint['speaker_classifier_state_dict'])\n",
    "    \n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    emotion_optimizer.load_state_dict(checkpoint['emotion_optimizer_state_dict'])\n",
    "    speaker_optimizer.load_state_dict(checkpoint['speaker_optimizer_state_dict'])\n",
    "    \n",
    "    emodb_dataset_train = EmoDBDataset2(ROOT,cv_index = fold ,split= 'train')\n",
    "    emodb_dataset_test = EmoDBDataset2(ROOT,cv_index = fold, split= 'test')\n",
    "    emodb_dataset_validate = EmoDBDataset2(ROOT,cv_index = fold, split= 'validate')\n",
    "    \n",
    "    TRAIN_BATCH_SIZE = len(emodb_dataset_train)\n",
    "    VALIDATE_BATCH_SIZE = len(emodb_dataset_validate)\n",
    "    TEST_BATCH_SIZE = len(emodb_dataset_test)\n",
    "    \n",
    "    emodb_train_loader = DataLoader(dataset=emodb_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    emodb_validate_loader = DataLoader(dataset=emodb_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    emodb_test_loader = DataLoader(dataset=emodb_dataset_test, batch_size= TEST_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    \n",
    "    encoder.eval()\n",
    "    emotion_classifier.eval()\n",
    "\n",
    "    lemotion_training_correct, lemotion_validating_correct, lemotion_testing_correct = 0 , 0 , 0\n",
    "    lspeaker_training_correct, lspeaker_validating_correct, lspeaker_testing_correct = 0 , 0 , 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "\n",
    "        # 1. Training Classification\n",
    "        for index, (features, emotion, speaker) in enumerate(emodb_train_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            lemotion_training_correct += (emotion_preds == emotion).sum() \n",
    "            lspeaker_training_correct += (speaker_preds == speaker).sum()\n",
    "        emotion_training_accuracy = lemotion_training_correct.item()/(len(emodb_train_loader)*TRAIN_BATCH_SIZE)\n",
    "        speaker_training_accuracy = lspeaker_training_correct.item()/(len(emodb_train_loader)*TRAIN_BATCH_SIZE)\n",
    "        print(\"\\nEmotion Training Correct: {}/{} \\nEmotion Training Accuracy: {:.5f}%\".format(lemotion_training_correct,(len(emodb_train_loader)*TRAIN_BATCH_SIZE),emotion_training_accuracy*100))\n",
    "        print(\"Speaker Training Correct: {}/{} \\nSpeaker Training Accuracy: {:.5f}%\".format(lspeaker_training_correct,(len(emodb_train_loader)*TRAIN_BATCH_SIZE),speaker_training_accuracy*100)) \n",
    "        \n",
    "        # 2. Validating Classification\n",
    "        for index, (features, emotion, speaker) in enumerate(emodb_validate_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            lemotion_validating_correct += (emotion_preds == emotion).sum() \n",
    "            lspeaker_validating_correct += (speaker_preds == speaker).sum()\n",
    "        emotion_validating_accuracy = lemotion_validating_correct.item()/(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "        speaker_validating_accuracy = lspeaker_validating_correct.item()/(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "        print(\"\\nEmotion Validating Correct: {}/{} \\nEmotion Validating Accuracy: {:.5f}%\".format(lemotion_validating_correct,(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE),emotion_validating_accuracy*100))\n",
    "        print(\"Speaker Validating Correct: {}/{} \\nSpeaker Validating Accuracy: {:.5f}%\".format(lspeaker_validating_correct,(len(emodb_validate_loader)*VALIDATE_BATCH_SIZE),speaker_validating_accuracy*100)) \n",
    "        \n",
    "\n",
    "        # 3. Testing Classification\n",
    "        for index, (features, emotion, speaker) in enumerate(emodb_test_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            lemotion_testing_correct += (emotion_preds == emotion).sum() \n",
    "            lspeaker_testing_correct += (speaker_preds == speaker).sum()\n",
    "        emotion_testing_accuracy = lemotion_testing_correct.item()/(len(emodb_test_loader)*TEST_BATCH_SIZE)\n",
    "        speaker_testing_accuracy = lspeaker_testing_correct.item()/(len(emodb_test_loader)*TEST_BATCH_SIZE)\n",
    "        print(\"\\nEmotion Testing Correct: {}/{} \\nEmotion Testing Accuracy: {:.5f}%\".format(lemotion_testing_correct,(len(emodb_test_loader)*TEST_BATCH_SIZE),emotion_testing_accuracy*100))\n",
    "        print(\"Speaker Testing Correct: {}/{} \\nSpeaker Testing Accuracy: {:.5f}%\".format(lspeaker_testing_correct,(len(emodb_test_loader)*TEST_BATCH_SIZE),speaker_testing_accuracy*100)) \n",
    "\n",
    "        lfold_emotion_training_accuracies.append(emotion_training_accuracy)\n",
    "        lfold_emotion_validating_accuracies.append(emotion_validating_accuracy)\n",
    "        lfold_emotion_testing_accuracies.append(emotion_testing_accuracy)\n",
    "        lfold_speaker_training_accuracies.append(speaker_training_accuracy)\n",
    "        lfold_speaker_validating_accuracies.append(speaker_validating_accuracy)\n",
    "        lfold_speaker_testing_accuracies.append(speaker_testing_accuracy)\n",
    "\n",
    "print('\\nSUMMARY:')\n",
    "print('\\nCV Emotion Training accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_training_accuracies), stdev(lfold_emotion_training_accuracies)))\n",
    "print('\\nCV Emotion Validating accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_validating_accuracies), stdev(lfold_emotion_validating_accuracies)))\n",
    "print('\\nCV Emotion Testing accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_testing_accuracies), stdev(lfold_emotion_testing_accuracies)))\n",
    "\n",
    "print('\\nCV Speaker Training accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_speaker_training_accuracies), stdev(lfold_speaker_training_accuracies)))\n",
    "print('\\nCV Speaker Validating accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_speaker_validating_accuracies), stdev(lfold_speaker_validating_accuracies)))\n",
    "print('\\nCV Speaker Testing accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_speaker_testing_accuracies), stdev(lfold_speaker_testing_accuracies)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2986.319654,
   "end_time": "2021-02-20T09:54:41.091325",
   "environment_variables": {},
   "exception": null,
   "input_path": "[DANN] EmoDB_MFCC_2L-CNN-GRU_32 training batch size-DAP1.25.ipynb",
   "output_path": "[DANN] EmoDB_MFCC_2L-CNN-GRU_32 training batch size-DAP1.25.ipynb",
   "parameters": {},
   "start_time": "2021-02-20T09:04:54.771671",
   "version": "2.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
