{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "french-nursery",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [13]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "binding-crime",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:28:47.119057Z",
     "iopub.status.busy": "2021-03-21T09:28:47.118295Z",
     "iopub.status.idle": "2021-03-21T09:29:07.572520Z",
     "shell.execute_reply": "2021-03-21T09:29:07.573270Z"
    },
    "papermill": {
     "duration": 20.519807,
     "end_time": "2021-03-21T09:29:07.573599",
     "exception": false,
     "start_time": "2021-03-21T09:28:47.053792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import scipy\n",
    "import math\n",
    "from scipy import signal\n",
    "import librosa.display\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "GAMMA = 1.25\n",
    "DATA = 'ravdess_MFCC'\n",
    "LAYERS = 3\n",
    "ROOT = './Dataset/{}'.format(DATA)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "MODEL_PATH = './model/{}_DANN_{}L-CNN-GRU_DAP{}_CV'.format(DATA,LAYERS,GAMMA)\n",
    "NPARR_PATH = './array/{}_DANN_{}L-CNN-GRU_DAP{}_CV accuracies.npz'.format(DATA,LAYERS,GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "consistent-christianity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:07.728736Z",
     "iopub.status.busy": "2021-03-21T09:29:07.725654Z",
     "iopub.status.idle": "2021-03-21T09:29:07.739246Z",
     "shell.execute_reply": "2021-03-21T09:29:07.739905Z"
    },
    "papermill": {
     "duration": 0.062871,
     "end_time": "2021-03-21T09:29:07.740132",
     "exception": false,
     "start_time": "2021-03-21T09:29:07.677261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to:42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(sd):\n",
    "    np.random.seed(sd)\n",
    "    random.seed(sd)\n",
    "    random.Random(sd)\n",
    "    torch.manual_seed(sd)\n",
    "    torch.cuda.manual_seed(sd)\n",
    "    torch.cuda.manual_seed_all(sd)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(\"Seed set to:{}\".format(sd))\n",
    "# also set worker_init_fn=np.random.seed(0),num_workers=0 in dataloader   \n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-connectivity",
   "metadata": {
    "papermill": {
     "duration": 0.022414,
     "end_time": "2021-03-21T09:29:07.895758",
     "exception": false,
     "start_time": "2021-03-21T09:29:07.873344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. CV Dataset for Ravdess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "strange-asset",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:08.097853Z",
     "iopub.status.busy": "2021-03-21T09:29:08.097138Z",
     "iopub.status.idle": "2021-03-21T09:29:08.119912Z",
     "shell.execute_reply": "2021-03-21T09:29:08.120892Z"
    },
    "papermill": {
     "duration": 0.143576,
     "end_time": "2021-03-21T09:29:08.121091",
     "exception": false,
     "start_time": "2021-03-21T09:29:07.977515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RavdessDataset2(object):\n",
    "    \"\"\"\n",
    "        Create a Dataset for RAVDESS. Each item is a tuple of the form:\n",
    "        (feature, emotion, speaker)\n",
    "    \"\"\" \n",
    "    \n",
    "    # Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)\n",
    "    _emotions = { '01': 0, '02': 1, '03': 2, '04': 3, '05': 4, '06': 5, '07': 6, '08': 7 }\n",
    " \n",
    "    _speaker = {'0'+ str(i+1): i for i in range(24) if i< 9}\n",
    "    _speaker.update({str(i+1): i for i in range(24) if i>= 9})\n",
    "\n",
    "    \n",
    "    def __init__(self, root, cv_index, split):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Directory containing the wav files\n",
    "            split(string): Either train, validate or test set\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.data = []\n",
    "        self.df = pd.DataFrame(self.data, columns=['Speaker', 'Emotion', 'File'])\n",
    "        self.allActors = ['0'+ str(i+1) for i in range(24)if i<9] + [str(i+1) for i in range(24)if i>=9]\n",
    "        \n",
    "        self.cv = { 0:  (['01','02'],['03','04']),\n",
    "                    1:  (['03','04'],['05','06']),\n",
    "                    2:  (['05','06'],['07','08']),\n",
    "                    3:  (['07','08'],['09','10']),\n",
    "                    4:  (['09','10'],['11','12']),\n",
    "                    5:  (['11','12'],['13','14']),\n",
    "                    6:  (['13','14'],['15','16']),\n",
    "                    7:  (['15','16'],['17','18']),\n",
    "                    8:  (['17','18'],['19','20']),\n",
    "                    9:  (['19','20'],['21','22']),\n",
    "                    10: (['21','22'],['23','24']),\n",
    "                    11: (['23','24'],['01','02'])\n",
    "                  }\n",
    "\n",
    "        # Iterate through all audio files\n",
    "        for root, dirs, files in os.walk(root):\n",
    "            for file in files: \n",
    "                \n",
    "                if split == 'train':\n",
    "                    if file[-5:-3] in [x for x in self.allActors if x not in (self.cv[cv_index][0]+ self.cv[cv_index][1])]:\n",
    "                        self.data.append([file[-5:-3],file[6:8],file])\n",
    "                        \n",
    "                elif split == 'validate':\n",
    "                    if file[-5:-3] in self.cv[cv_index][0]:\n",
    "                        self.data.append([file[-5:-3],file[6:8],file])\n",
    "                        \n",
    "                elif split == 'test':\n",
    "                    if file[-5:-3] in self.cv[cv_index][1]:\n",
    "                        self.data.append([file[-5:-3],file[6:8],file])\n",
    "                else:\n",
    "                    print(\"Error: Split can only be train, validate or test!\")\n",
    "\n",
    "        # Convert data to pandas dataframe\n",
    "        self.df = pd.DataFrame(self.data, columns=['Speaker', 'Emotion', 'File'])\n",
    "\n",
    "        # Map emotion labels to numeric values\n",
    "        self.df['Emotion'] = self.df['Emotion'].map(self._emotions).astype(np.long)\n",
    "        self.df['Speaker'] = self.df['Speaker'].map(self._speaker).astype(np.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        file_name = os.path.join(self.root, self.df.loc[idx, 'File'])\n",
    "        feature = torch.load(file_name )\n",
    "        emotion = self.df.loc[idx, 'Emotion']\n",
    "        speaker = self.df.loc[idx, 'Speaker']\n",
    "        \n",
    "        # return a tuple instead of a dictionary\n",
    "        sample = (feature,emotion,speaker)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "convinced-gender",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:08.313376Z",
     "iopub.status.busy": "2021-03-21T09:29:08.312094Z",
     "iopub.status.idle": "2021-03-21T09:29:08.313989Z",
     "shell.execute_reply": "2021-03-21T09:29:08.312744Z"
    },
    "papermill": {
     "duration": 0.132149,
     "end_time": "2021-03-21T09:29:08.328915",
     "exception": false,
     "start_time": "2021-03-21T09:29:08.196766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initialize RavdessDataset\n",
    "# ravdess_dataset_train = RavdessDataset2(ROOT,cv_index = 4,split= 'train')\n",
    "# ravdess_dataset_test = RavdessDataset2(ROOT,cv_index = 4,split= 'test')\n",
    "# ravdess_dataset_validate = RavdessDataset2(ROOT,cv_index = 4, split= 'validate')\n",
    "\n",
    "# # To view dataframe, uncomment below: \n",
    "# ravdess_dataset_train.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "arbitrary-exhibit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:08.540070Z",
     "iopub.status.busy": "2021-03-21T09:29:08.539037Z",
     "iopub.status.idle": "2021-03-21T09:29:08.541561Z",
     "shell.execute_reply": "2021-03-21T09:29:08.540850Z"
    },
    "papermill": {
     "duration": 0.095374,
     "end_time": "2021-03-21T09:29:08.541743",
     "exception": false,
     "start_time": "2021-03-21T09:29:08.446369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# TRAIN_BATCH_SIZE = 16\n",
    "# VALIDATE_BATCH_SIZE = len(ravdess_dataset_validate)\n",
    "# TEST_BATCH_SIZE = len(ravdess_dataset_test)\n",
    "# ravdess_train_loader = DataLoader(dataset=ravdess_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=2, pin_memory=True)\n",
    "# ravdess_validate_loader = DataLoader(dataset=ravdess_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=2, pin_memory=True)\n",
    "# ravdess_test_loader = DataLoader(dataset=ravdess_dataset_test, batch_size= TEST_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "statutory-parking",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:08.717877Z",
     "iopub.status.busy": "2021-03-21T09:29:08.716543Z",
     "iopub.status.idle": "2021-03-21T09:29:08.719795Z",
     "shell.execute_reply": "2021-03-21T09:29:08.717138Z"
    },
    "papermill": {
     "duration": 0.12881,
     "end_time": "2021-03-21T09:29:08.732705",
     "exception": false,
     "start_time": "2021-03-21T09:29:08.603895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next(iter(ravdess_train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-azerbaijan",
   "metadata": {
    "papermill": {
     "duration": 0.096221,
     "end_time": "2021-03-21T09:29:08.908166",
     "exception": false,
     "start_time": "2021-03-21T09:29:08.811945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Network Architectures and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-fluid",
   "metadata": {
    "papermill": {
     "duration": 0.095805,
     "end_time": "2021-03-21T09:29:09.057425",
     "exception": false,
     "start_time": "2021-03-21T09:29:08.961620",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dense-gates",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:09.243843Z",
     "iopub.status.busy": "2021-03-21T09:29:09.243143Z",
     "iopub.status.idle": "2021-03-21T09:29:09.259348Z",
     "shell.execute_reply": "2021-03-21T09:29:09.259893Z"
    },
    "papermill": {
     "duration": 0.166365,
     "end_time": "2021-03-21T09:29:09.260097",
     "exception": false,
     "start_time": "2021-03-21T09:29:09.093732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        # For same padding, P = ((S-1)*W-S+F)/2, with F = filter size, S = stride. If stride = 1, P = (F-1)/2\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(2, 2),padding=1,bias=False),  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout2d(p=0.1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),  \n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(2, 2),padding=1,bias= False),  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout2d(p=0.1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)), \n",
    "            \n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(2, 2),padding=1,bias=False),  \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout2d(p=0.1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "                     \n",
    "        )\n",
    "        \n",
    "        self.GRU = nn.GRU(input_size= 384, hidden_size = 256 ,batch_first = True, bidirectional = True) #128\n",
    "        self.FC = nn.Linear(in_features = 512, out_features = 512) \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv_layers(x)\n",
    "        # print(\"1-LFLB shape: {}\".format(x.shape))                     # batch_size, c_out, freq, time since we are using spectogram\n",
    "        x = x.view(x.size(0),-1,x.size(3)) \n",
    "        # print(\"Batch size, features, seq shape: {}\".format(x.shape))  # batch_size, features= c_out* freq , seq_len= time\n",
    "        x = x.permute(0,2,1)\n",
    "        # print(\"Batch size, seq, features shape: {}\".format(x.shape))  # batch_size, seq, features\n",
    "        output, hn = self.GRU(x)\n",
    "        # print(\"GRU output shape: {}\".format(output.shape))            # batch_size, seq, hidden size * 2 \n",
    "        x = self.FC(output)\n",
    "        # print(\"FC shape: {}\".format(x.shape)) # 32 * 141 * 512\n",
    "        mean = torch.mean(x,1)\n",
    "        stdev = torch.std(x,1)\n",
    "        x = torch.cat((mean,stdev),1)\n",
    "        #print(\"Statistical pooling shape: {}\".format(x.shape)) # 32 * 1024\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fixed-damage",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:09.372923Z",
     "iopub.status.busy": "2021-03-21T09:29:09.372241Z",
     "iopub.status.idle": "2021-03-21T09:29:09.379021Z",
     "shell.execute_reply": "2021-03-21T09:29:09.379510Z"
    },
    "papermill": {
     "duration": 0.060249,
     "end_time": "2021-03-21T09:29:09.379698",
     "exception": false,
     "start_time": "2021-03-21T09:29:09.319449",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionClassifier,self).__init__()\n",
    "        self.label_classifier = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(1024, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 8),\n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.label_classifier(x)\n",
    "        return F.softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "intelligent-outside",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:09.543520Z",
     "iopub.status.busy": "2021-03-21T09:29:09.541948Z",
     "iopub.status.idle": "2021-03-21T09:29:09.544303Z",
     "shell.execute_reply": "2021-03-21T09:29:09.542800Z"
    },
    "papermill": {
     "duration": 0.103844,
     "end_time": "2021-03-21T09:29:09.544463",
     "exception": false,
     "start_time": "2021-03-21T09:29:09.440619",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpeakerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpeakerClassifier,self).__init__()\n",
    "        self.label_classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 24),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.label_classifier(x)\n",
    "        return F.softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-legislature",
   "metadata": {
    "papermill": {
     "duration": 0.024928,
     "end_time": "2021-03-21T09:29:09.619993",
     "exception": false,
     "start_time": "2021-03-21T09:29:09.595065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ultimate-coalition",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:09.762368Z",
     "iopub.status.busy": "2021-03-21T09:29:09.761605Z",
     "iopub.status.idle": "2021-03-21T09:29:09.766051Z",
     "shell.execute_reply": "2021-03-21T09:29:09.765517Z"
    },
    "papermill": {
     "duration": 0.110008,
     "end_time": "2021-03-21T09:29:09.766205",
     "exception": false,
     "start_time": "2021-03-21T09:29:09.656197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def domain_adaptation_parameter(p):\n",
    "    lambda_p = 2. / (1. + np.exp(-GAMMA*p)) - 1\n",
    "    return lambda_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "separate-preparation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:10.057792Z",
     "iopub.status.busy": "2021-03-21T09:29:10.046423Z",
     "iopub.status.idle": "2021-03-21T09:29:10.060312Z",
     "shell.execute_reply": "2021-03-21T09:29:10.059737Z"
    },
    "papermill": {
     "duration": 0.171699,
     "end_time": "2021-03-21T09:29:10.060454",
     "exception": false,
     "start_time": "2021-03-21T09:29:09.888755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_parameters(PATH):\n",
    "    torch.save({\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'emotion_classifier_state_dict': emotion_classifier.state_dict(),\n",
    "                'speaker_classifier_state_dict': speaker_classifier.state_dict(),\n",
    "                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                'emotion_optimizer_state_dict': emotion_optimizer.state_dict(),\n",
    "                'speaker_optimizer_state_dict': speaker_optimizer.state_dict(),\n",
    "                }, PATH)\n",
    "    print(\"Models' parameters and optimisers' parameters saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "amended-warrant",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:10.211094Z",
     "iopub.status.busy": "2021-03-21T09:29:10.209914Z",
     "iopub.status.idle": "2021-03-21T09:29:10.211648Z",
     "shell.execute_reply": "2021-03-21T09:29:10.210524Z"
    },
    "papermill": {
     "duration": 0.091049,
     "end_time": "2021-03-21T09:29:10.211799",
     "exception": false,
     "start_time": "2021-03-21T09:29:10.120750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_accuracies_and_losses(PATH):\n",
    "    np.savez(PATH, \n",
    "         emoClassLoss = fold_emotion_class_losses,\n",
    "         emoTrain_acc = fold_emotion_training_accuracies, \n",
    "         emoValidate_acc = fold_emotion_validating_accuracies,\n",
    "         spkClassLoss = fold_speaker_class_losses,\n",
    "         spkTrain_acc = fold_speaker_training_accuracies,\n",
    "         spkValidate_acc = fold_speaker_validating_accuracies,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-roads",
   "metadata": {
    "papermill": {
     "duration": 0.113696,
     "end_time": "2021-03-21T09:29:10.406504",
     "exception": false,
     "start_time": "2021-03-21T09:29:10.292808",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-fence",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lyric-elephant",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-21T09:29:10.674714Z",
     "iopub.status.busy": "2021-03-21T09:29:10.640303Z",
     "iopub.status.idle": "2021-03-21T09:29:54.137200Z",
     "shell.execute_reply": "2021-03-21T09:29:54.134939Z"
    },
    "papermill": {
     "duration": 43.618541,
     "end_time": "2021-03-21T09:29:54.137622",
     "exception": true,
     "start_time": "2021-03-21T09:29:10.519081",
     "status": "failed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current Fold: 0 | Epoch: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 15.90 GiB total capacity; 61.66 MiB already allocated; 27.75 MiB free; 82.00 MiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7d9eb9decb70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;31m# Calculate speaker and emotion classification prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mconv_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0memotion_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memotion_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0memotion_class_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-d1d6eed5546a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;31m# print(\"1-LFLB shape: {}\".format(x.shape))                     # batch_size, c_out, freq, time since we are using spectogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch2/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2147\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 15.90 GiB total capacity; 61.66 MiB already allocated; 27.75 MiB free; 82.00 MiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.optim as optim\n",
    "import pylab as plt\n",
    "\n",
    "# Read this to understand how GRL works: \n",
    "# https://christineai.blog/category/domain-adaptation/\n",
    "\n",
    "FOLDS = 12\n",
    "EPOCHS = 100\n",
    "\n",
    "############### To comment this section out if disrupted #############\n",
    "fold_emotion_class_losses = np.zeros((FOLDS,EPOCHS))\n",
    "fold_emotion_training_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "fold_emotion_validating_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "fold_speaker_class_losses = np.zeros((FOLDS,EPOCHS))\n",
    "fold_speaker_training_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "fold_speaker_validating_accuracies = np.zeros((FOLDS,EPOCHS))\n",
    "\n",
    "\n",
    "######### To uncomment if trying to continue disrupted training ######\n",
    "# fold_emotion_class_losses, fold_emotion_training_accuracies, fold_emotion_validating_accuracies， fold_speaker_class_losses，fold_speaker_training_accuracies，fold_speaker_validating_accuracies= np.load(NPARR_PATH)\n",
    "\n",
    "###################################################################\n",
    "\n",
    "# 12-fold cross validation\n",
    "for fold in range(0,FOLDS):\n",
    "\n",
    "    # Selecting CPU or GPU\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    # Selecting the type of encoder, label classifier\n",
    "    encoder = FeatureExtractor().to(DEVICE).train()\n",
    "    emotion_classifier = EmotionClassifier().to(DEVICE).train()\n",
    "    speaker_classifier = SpeakerClassifier().to(DEVICE).train()\n",
    "\n",
    "    # Optimizer \n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters())\n",
    "    emotion_optimizer = torch.optim.Adam(emotion_classifier.parameters())\n",
    "    speaker_optimizer = torch.optim.Adam(speaker_classifier.parameters())\n",
    "\n",
    "    cross_entropy_loss = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    \n",
    "    ravdess_dataset_train = RavdessDataset2(ROOT,cv_index = fold ,split= 'train')\n",
    "    ravdess_dataset_test = RavdessDataset2(ROOT,cv_index = fold, split= 'test')\n",
    "    ravdess_dataset_validate = RavdessDataset2(ROOT,cv_index = fold, split= 'validate')\n",
    "    \n",
    "    # DANN should be trained on labelled data from the source domain and unlabelled data from the target domain\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    ravdess_train_loader = DataLoader(dataset=ravdess_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=True,worker_init_fn=np.random.seed(42),num_workers=4, pin_memory= True)\n",
    "    \n",
    "    # For evaluation purposes\n",
    "    VALIDATE_BATCH_SIZE = len(ravdess_dataset_validate)\n",
    "    ravdess_validate_loader = DataLoader(dataset=ravdess_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=4, pin_memory= True)\n",
    "    \n",
    "    epoch_emotion_class_losses = []\n",
    "    emotion_training_accuracies = []\n",
    "    emotion_validating_accuracies = []\n",
    "\n",
    "    epoch_speaker_class_losses = []\n",
    "    speaker_training_accuracies = []\n",
    "    speaker_validating_accuracies = []\n",
    "\n",
    "    STEP = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"\\nCurrent Fold: {} | Epoch: {}\".format(fold, epoch))\n",
    "\n",
    "        completed_start_steps = epoch * len(ravdess_train_loader)\n",
    "        total_steps = EPOCHS * len(ravdess_train_loader)\n",
    "\n",
    "        batch_emotion_class_losses = []\n",
    "        batch_speaker_class_losses = []\n",
    "\n",
    "\n",
    "        for batch_idx, (feature, emotion, speaker) in enumerate(ravdess_train_loader):\n",
    "\n",
    "            # Assigned to DEVICE. \n",
    "            features, emotion, speaker = feature.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE)\n",
    "            \n",
    "            # Computing the training progress\n",
    "            p = (batch_idx + completed_start_steps) / total_steps\n",
    "            lambda_p = domain_adaptation_parameter(p)\n",
    "\n",
    "            # Calculate speaker and emotion classification prediction \n",
    "            conv_features = encoder(features)\n",
    "            emotion_preds = emotion_classifier(conv_features)\n",
    "            emotion_class_loss = cross_entropy_loss(emotion_preds, emotion)\n",
    "            speaker_preds = speaker_classifier(conv_features)\n",
    "            speaker_class_loss = cross_entropy_loss(speaker_preds, speaker)\n",
    "\n",
    "            # Calculate total loss\n",
    "            total_loss = emotion_class_loss - lambda_p * speaker_class_loss \n",
    "\n",
    "            # Clear the gradient to prevent gradient accumulation\n",
    "            encoder.zero_grad(set_to_none= True)\n",
    "            emotion_classifier.zero_grad(set_to_none= True)\n",
    "            speaker_classifier.zero_grad(set_to_none= True)\n",
    "\n",
    "            # Computing the gradient\n",
    "            total_loss.backward()\n",
    "\n",
    "            # Update the weight\n",
    "            emotion_optimizer.step()\n",
    "            speaker_optimizer.step()\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "            batch_emotion_class_losses.append(emotion_class_loss.detach())\n",
    "            batch_speaker_class_losses.append(speaker_class_loss.detach())\n",
    "\n",
    "\n",
    "        # Enter evaluation mode at the end of each epoch\n",
    "        encoder.eval()\n",
    "        emotion_classifier.eval()\n",
    "        speaker_classifier.eval()\n",
    "\n",
    "        emotion_training_correct, emotion_validating_correct, speaker_training_correct, speaker_validating_correct = 0 , 0 , 0 , 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "        # 1. Training Classification\n",
    "          for index, (features, emotion, speaker) in enumerate(ravdess_train_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            emotion_training_correct += (emotion_preds == emotion).sum() \n",
    "            speaker_training_correct += (speaker_preds == speaker).sum() \n",
    "          #source_accuracy = torch.true_divide(source_correct, len(svhn_test_loader.dataset))\n",
    "          emotion_training_accuracy = emotion_training_correct.item()/(len(ravdess_train_loader)*TRAIN_BATCH_SIZE)\n",
    "          emotion_training_accuracies.append(emotion_training_accuracy)\n",
    "          speaker_training_accuracy = speaker_training_correct.item()/(len(ravdess_train_loader)*TRAIN_BATCH_SIZE)\n",
    "          speaker_training_accuracies.append(speaker_training_accuracy)\n",
    "          print(\"Emotion Training Correct: {}/{} \\nEmotion Training Accuracy: {:.5f}%\".format(emotion_training_correct,(len(ravdess_train_loader)*TRAIN_BATCH_SIZE),emotion_training_accuracy*100))\n",
    "          print(\"Speaker Training Correct: {}/{} \\nSpeaker Training Accuracy: {:.5f}%\".format(speaker_training_correct,(len(ravdess_train_loader)*TRAIN_BATCH_SIZE),speaker_training_accuracy*100))\n",
    "\n",
    "        # 2. Validating Classification\n",
    "          for index, (features, emotion, speaker) in enumerate(ravdess_validate_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            emotion_validating_correct += (emotion_preds == emotion).sum() \n",
    "            speaker_validating_correct += (speaker_preds == speaker).sum() \n",
    "          #source_accuracy = torch.true_divide(source_correct, len(svhn_test_loader.dataset))\n",
    "          emotion_validating_accuracy = emotion_validating_correct.item()/(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "          emotion_validating_accuracies.append(emotion_validating_accuracy)\n",
    "          speaker_validating_accuracy = speaker_validating_correct.item()/(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "          speaker_validating_accuracies.append(speaker_validating_accuracy)\n",
    "          print(\"\\nEmotion Validating Correct: {}/{} \\nEmotion Validating Accuracy: {:.5f}%\".format(emotion_validating_correct,(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE),emotion_validating_accuracy*100))\n",
    "          print(\"Speaker Validating Correct: {}/{} \\nSpeaker Validating Accuracy: {:.5f}%\".format(speaker_validating_correct,(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE),speaker_validating_accuracy*100))\n",
    "\n",
    "          if (len(emotion_validating_accuracies)> 1 and emotion_validating_accuracy >= max(emotion_validating_accuracies[:-1])):\n",
    "                save_parameters(MODEL_PATH + 'fold' + str(fold))\n",
    "\n",
    "\n",
    "        encoder.train()\n",
    "        emotion_classifier.train()\n",
    "        speaker_classifier.train()\n",
    "\n",
    "\n",
    "        epoch_emotion_class_loss = torch.mean(torch.stack(batch_emotion_class_losses), dim=0)\n",
    "        epoch_emotion_class_losses.append(epoch_emotion_class_loss)\n",
    "        epoch_speaker_class_loss = torch.mean(torch.stack(batch_speaker_class_losses), dim=0)\n",
    "        epoch_speaker_class_losses.append(epoch_speaker_class_loss)\n",
    "        \n",
    "    fold_emotion_class_losses[fold] = epoch_emotion_class_losses\n",
    "    fold_emotion_training_accuracies[fold] = emotion_training_accuracies\n",
    "    fold_emotion_validating_accuracies[fold] = emotion_validating_accuracies\n",
    "    fold_speaker_class_losses[fold] = epoch_speaker_class_losses\n",
    "    fold_speaker_training_accuracies[fold] = speaker_training_accuracies\n",
    "    fold_speaker_validating_accuracies[fold] = speaker_validating_accuracies\n",
    "    \n",
    "    save_accuracies_and_losses(NPARR_PATH)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-xerox",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-11T15:30:05.328458Z",
     "iopub.status.busy": "2021-03-11T15:30:05.285032Z",
     "iopub.status.idle": "2021-03-11T15:30:06.319027Z",
     "shell.execute_reply": "2021-03-11T15:30:06.319419Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.title('Losses vs. epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Losses')\n",
    "\n",
    "for i in range(FOLDS):\n",
    "    plt.plot(range(EPOCHS), fold_emotion_class_losses[i],label='emotion classification loss fold {}'.format(i))\n",
    "    plt.plot(range(EPOCHS), fold_speaker_class_losses[i],label='speaker classification loss fold {}'.format(i))\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-mission",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-11T15:30:07.373224Z",
     "iopub.status.busy": "2021-03-11T15:30:07.372574Z",
     "iopub.status.idle": "2021-03-11T15:30:07.374792Z",
     "shell.execute_reply": "2021-03-11T15:30:07.374217Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_accuracies_vs_epochs(fold):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.title('Accuracies vs. epochs')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.plot(range(EPOCHS), fold_emotion_training_accuracies[fold],label='emotion_training_accuracies fold {}'.format(fold))\n",
    "    plt.plot(range(EPOCHS), fold_emotion_validating_accuracies[fold],label='emotion_validating_accuracies fold {}'.format(fold))\n",
    "    plt.plot(range(EPOCHS), fold_speaker_training_accuracies[fold],label='speaker_training_accuracies fold {}'.format(fold))\n",
    "    plt.plot(range(EPOCHS), fold_speaker_validating_accuracies[fold],label='speaker_validating_accuracies fold {}'.format(fold))\n",
    "    plt.legend(loc='best')\n",
    "    plt.show() \n",
    "    print(\"Maximum emotion training accuracy:{:.2f}%\".format(max(fold_emotion_training_accuracies[fold])*100))\n",
    "    print(\"Maximum emotion validating accuracy:{:.2f}%\".format(max(fold_emotion_validating_accuracies[fold])*100))\n",
    "    print(\"Maximum speaker training accuracy:{:.2f}%\".format(max(fold_speaker_training_accuracies[fold])*100))\n",
    "    print(\"Maximum speaker validating accuracy:{:.2f}%\".format(max(fold_speaker_validating_accuracies[fold])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-capacity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-11T15:30:08.449697Z",
     "iopub.status.busy": "2021-03-11T15:30:08.448907Z",
     "iopub.status.idle": "2021-03-11T15:30:13.106108Z",
     "shell.execute_reply": "2021-03-11T15:30:13.106465Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(FOLDS):\n",
    "    plot_accuracies_vs_epochs(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-exploration",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. Loading and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-involvement",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-11T15:30:15.330336Z",
     "iopub.status.busy": "2021-03-11T15:30:15.326989Z",
     "iopub.status.idle": "2021-03-11T15:30:19.571347Z",
     "shell.execute_reply": "2021-03-11T15:30:19.570954Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from statistics import mean , stdev\n",
    "\n",
    "\n",
    "lfold_emotion_training_accuracies, lfold_emotion_validating_accuracies, lfold_emotion_testing_accuracies = [] , [] , []\n",
    "lfold_speaker_training_accuracies, lfold_speaker_validating_accuracies, lfold_speaker_testing_accuracies = [] , [] , []\n",
    "\n",
    "for fold in range(12):\n",
    "    print(\"\\nEvaluation for fold {}\".format(fold))\n",
    "    checkpoint = torch.load(MODEL_PATH + 'fold' + str(fold))\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    encoder = FeatureExtractor().to(DEVICE).train()\n",
    "    emotion_classifier = EmotionClassifier().to(DEVICE).train()\n",
    "    speaker_classifier = SpeakerClassifier().to(DEVICE)\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters())\n",
    "    emotion_optimizer = torch.optim.Adam(emotion_classifier.parameters())\n",
    "    speaker_optimizer = torch.optim.Adam(speaker_classifier.parameters())\n",
    "    \n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    emotion_classifier.load_state_dict(checkpoint['emotion_classifier_state_dict'])\n",
    "    speaker_classifier.load_state_dict(checkpoint['speaker_classifier_state_dict'])\n",
    "    \n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    emotion_optimizer.load_state_dict(checkpoint['emotion_optimizer_state_dict'])\n",
    "    speaker_optimizer.load_state_dict(checkpoint['speaker_optimizer_state_dict'])\n",
    "    \n",
    "    ravdess_dataset_train = RavdessDataset2(ROOT,cv_index = fold ,split= 'train')\n",
    "    ravdess_dataset_test = RavdessDataset2(ROOT,cv_index = fold, split= 'test')\n",
    "    ravdess_dataset_validate = RavdessDataset2(ROOT,cv_index = fold, split= 'validate')\n",
    "    \n",
    "    TRAIN_BATCH_SIZE = len(ravdess_dataset_train)\n",
    "    VALIDATE_BATCH_SIZE = len(ravdess_dataset_validate)\n",
    "    TEST_BATCH_SIZE = len(ravdess_dataset_test)\n",
    "    \n",
    "    ravdess_train_loader = DataLoader(dataset=ravdess_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    ravdess_validate_loader = DataLoader(dataset=ravdess_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    ravdess_test_loader = DataLoader(dataset=ravdess_dataset_test, batch_size= TEST_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    \n",
    "    encoder.eval()\n",
    "    emotion_classifier.eval()\n",
    "\n",
    "    lemotion_training_correct, lemotion_validating_correct, lemotion_testing_correct = 0 , 0 , 0\n",
    "    lspeaker_training_correct, lspeaker_validating_correct, lspeaker_testing_correct = 0 , 0 , 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "\n",
    "        # 1. Training Classification\n",
    "        for index, (features, emotion, speaker) in enumerate(ravdess_train_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            lemotion_training_correct += (emotion_preds == emotion).sum() \n",
    "            lspeaker_training_correct += (speaker_preds == speaker).sum()\n",
    "        emotion_training_accuracy = lemotion_training_correct.item()/(len(ravdess_train_loader)*TRAIN_BATCH_SIZE)\n",
    "        speaker_training_accuracy = lspeaker_training_correct.item()/(len(ravdess_train_loader)*TRAIN_BATCH_SIZE)\n",
    "        print(\"\\nEmotion Training Correct: {}/{} \\nEmotion Training Accuracy: {:.5f}%\".format(lemotion_training_correct,(len(ravdess_train_loader)*TRAIN_BATCH_SIZE),emotion_training_accuracy*100))\n",
    "        print(\"Speaker Training Correct: {}/{} \\nSpeaker Training Accuracy: {:.5f}%\".format(lspeaker_training_correct,(len(ravdess_train_loader)*TRAIN_BATCH_SIZE),speaker_training_accuracy*100)) \n",
    "        \n",
    "        # 2. Validating Classification\n",
    "        for index, (features, emotion, speaker) in enumerate(ravdess_validate_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            lemotion_validating_correct += (emotion_preds == emotion).sum() \n",
    "            lspeaker_validating_correct += (speaker_preds == speaker).sum()\n",
    "        emotion_validating_accuracy = lemotion_validating_correct.item()/(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "        speaker_validating_accuracy = lspeaker_validating_correct.item()/(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "        print(\"\\nEmotion Validating Correct: {}/{} \\nEmotion Validating Accuracy: {:.5f}%\".format(lemotion_validating_correct,(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE),emotion_validating_accuracy*100))\n",
    "        print(\"Speaker Validating Correct: {}/{} \\nSpeaker Validating Accuracy: {:.5f}%\".format(lspeaker_validating_correct,(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE),speaker_validating_accuracy*100)) \n",
    "        \n",
    "\n",
    "        # 3. Testing Classification\n",
    "        for index, (features, emotion, speaker) in enumerate(ravdess_test_loader):\n",
    "            features, emotion, speaker = features.to(DEVICE),emotion.to(DEVICE), speaker.to(DEVICE) \n",
    "            conv_features = encoder(features)\n",
    "            emotion_output = emotion_classifier(conv_features)\n",
    "            speaker_output = speaker_classifier(conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            _, speaker_preds = torch.max(speaker_output,1)\n",
    "            lemotion_testing_correct += (emotion_preds == emotion).sum() \n",
    "            lspeaker_testing_correct += (speaker_preds == speaker).sum()\n",
    "        emotion_testing_accuracy = lemotion_testing_correct.item()/(len(ravdess_test_loader)*TEST_BATCH_SIZE)\n",
    "        speaker_testing_accuracy = lspeaker_testing_correct.item()/(len(ravdess_test_loader)*TEST_BATCH_SIZE)\n",
    "        print(\"\\nEmotion Testing Correct: {}/{} \\nEmotion Testing Accuracy: {:.5f}%\".format(lemotion_testing_correct,(len(ravdess_test_loader)*TEST_BATCH_SIZE),emotion_testing_accuracy*100))\n",
    "        print(\"Speaker Testing Correct: {}/{} \\nSpeaker Testing Accuracy: {:.5f}%\".format(lspeaker_testing_correct,(len(ravdess_test_loader)*TEST_BATCH_SIZE),speaker_testing_accuracy*100)) \n",
    "\n",
    "        lfold_emotion_training_accuracies.append(emotion_training_accuracy)\n",
    "        lfold_emotion_validating_accuracies.append(emotion_validating_accuracy)\n",
    "        lfold_emotion_testing_accuracies.append(emotion_testing_accuracy)\n",
    "        lfold_speaker_training_accuracies.append(speaker_training_accuracy)\n",
    "        lfold_speaker_validating_accuracies.append(speaker_validating_accuracy)\n",
    "        lfold_speaker_testing_accuracies.append(speaker_testing_accuracy)\n",
    "\n",
    "print('\\nSUMMARY:')\n",
    "print('\\nCV Emotion Training accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_training_accuracies), stdev(lfold_emotion_training_accuracies)))\n",
    "print('\\nCV Emotion Validating accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_validating_accuracies), stdev(lfold_emotion_validating_accuracies)))\n",
    "print('\\nCV Emotion Testing accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_testing_accuracies), stdev(lfold_emotion_testing_accuracies)))\n",
    "\n",
    "print('\\nCV Speaker Training accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_speaker_training_accuracies), stdev(lfold_speaker_training_accuracies)))\n",
    "print('\\nCV Speaker Validating accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_speaker_validating_accuracies), stdev(lfold_speaker_validating_accuracies)))\n",
    "print('\\nCV Speaker Testing accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_speaker_testing_accuracies), stdev(lfold_speaker_testing_accuracies)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 79.840588,
   "end_time": "2021-03-21T09:29:59.626045",
   "environment_variables": {},
   "exception": true,
   "input_path": "[DANN] Ravdess_MFCC_3L-CNN-GRU_DAP0-pool.ipynb",
   "output_path": "[DANN] Ravdess_MFCC_3L-CNN-GRU_DAP0-pool.ipynb",
   "parameters": {},
   "start_time": "2021-03-21T09:28:39.785457",
   "version": "2.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
