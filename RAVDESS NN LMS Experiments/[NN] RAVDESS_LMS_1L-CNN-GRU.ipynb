{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:16:58.183902Z",
     "iopub.status.busy": "2021-02-05T14:16:58.183335Z",
     "iopub.status.idle": "2021-02-05T14:17:11.520442Z",
     "shell.execute_reply": "2021-02-05T14:17:11.520949Z"
    },
    "papermill": {
     "duration": 13.364796,
     "end_time": "2021-02-05T14:17:11.521257",
     "exception": false,
     "start_time": "2021-02-05T14:16:58.156461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryanleow/FYP/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE = False\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import scipy\n",
    "import math\n",
    "from scipy import signal\n",
    "import librosa.display\n",
    "import torch.nn as nn\n",
    "import statistics\n",
    "from torch.utils.data import DataLoader\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "MODEL_PATH = './model/ravdess_LMS_NN_1L-CNN-GRU_32_CV'\n",
    "NPARR_PATH = './array/ravdess_LMS_NN_1L-CNN-GRU_32_CV accuracies.npz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:11.571847Z",
     "iopub.status.busy": "2021-02-05T14:17:11.571221Z",
     "iopub.status.idle": "2021-02-05T14:17:11.579830Z",
     "shell.execute_reply": "2021-02-05T14:17:11.579316Z"
    },
    "papermill": {
     "duration": 0.035143,
     "end_time": "2021-02-05T14:17:11.579974",
     "exception": false,
     "start_time": "2021-02-05T14:17:11.544831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed set to:42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(sd):\n",
    "    np.random.seed(sd)\n",
    "    random.seed(sd)\n",
    "    random.Random(sd)\n",
    "    torch.manual_seed(sd)\n",
    "    torch.cuda.manual_seed(sd)\n",
    "    torch.cuda.manual_seed_all(sd)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    print(\"Seed set to:{}\".format(sd))\n",
    "# also set worker_init_fn=np.random.seed(0),num_workers=4, pin_memory=True in dataloader   \n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02513,
     "end_time": "2021-02-05T14:17:11.632205",
     "exception": false,
     "start_time": "2021-02-05T14:17:11.607075",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. CV Dataset for Ravdess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:11.701240Z",
     "iopub.status.busy": "2021-02-05T14:17:11.700567Z",
     "iopub.status.idle": "2021-02-05T14:17:11.702684Z",
     "shell.execute_reply": "2021-02-05T14:17:11.702138Z"
    },
    "papermill": {
     "duration": 0.046053,
     "end_time": "2021-02-05T14:17:11.702858",
     "exception": false,
     "start_time": "2021-02-05T14:17:11.656805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RavdessDataset2(object):\n",
    "    \"\"\"\n",
    "        Create a Dataset for RAVDESS. Each item is a tuple of the form:\n",
    "        (waveform, sample_rate, emotion, speaker)\n",
    "    \"\"\" \n",
    "    \n",
    "    # Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)\n",
    "    _emotions = { '01': 0, '02': 1, '03': 2, '04': 3, '05': 4, '06': 5, '07': 6, '08': 7 }\n",
    " \n",
    "    _speaker = {'Actor_0'+ str(i+1): i for i in range(24) if i< 9}\n",
    "    _speaker.update({'Actor_'+ str(i+1): i for i in range(24) if i>= 9})\n",
    "    \n",
    "    rev_speaker = {i: 'Actor_0' + str(i+1) for i in range(24) if i<9}\n",
    "    rev_speaker.update({i: 'Actor_' + str(i+1) for i in range(24) if i>=9})\n",
    "    \n",
    "    def __init__(self, root, cv_index, split):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Directory containing the wav files\n",
    "            split(string): Either train, validate or test set\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.data = []\n",
    "        self.df = pd.DataFrame(self.data, columns=['Speaker', 'Emotion', 'File'])\n",
    "        self.allActors = ['Actor_0'+ str(i+1) for i in range(24)if i<9] + ['Actor_'+ str(i+1) for i in range(24)if i>=9]\n",
    "        \n",
    "        self.cv = { 0:  (['Actor_01','Actor_02'],['Actor_03','Actor_04']),\n",
    "                    1:  (['Actor_03','Actor_04'],['Actor_05','Actor_06']),\n",
    "                    2:  (['Actor_05','Actor_06'],['Actor_07','Actor_08']),\n",
    "                    3:  (['Actor_07','Actor_08'],['Actor_09','Actor_10']),\n",
    "                    4:  (['Actor_09','Actor_10'],['Actor_11','Actor_12']),\n",
    "                    5:  (['Actor_11','Actor_12'],['Actor_13','Actor_14']),\n",
    "                    6:  (['Actor_13','Actor_14'],['Actor_15','Actor_16']),\n",
    "                    7:  (['Actor_15','Actor_16'],['Actor_17','Actor_18']),\n",
    "                    8:  (['Actor_17','Actor_18'],['Actor_19','Actor_20']),\n",
    "                    9:  (['Actor_19','Actor_20'],['Actor_21','Actor_22']),\n",
    "                    10: (['Actor_21','Actor_22'],['Actor_23','Actor_24']),\n",
    "                    11: (['Actor_23','Actor_24'],['Actor_01','Actor_02'])\n",
    "                  }\n",
    "\n",
    "        # Iterate through all audio files\n",
    "        for root, dirs, files in os.walk(root):\n",
    "            if split == 'train':\n",
    "                if root[-8:] in [x for x in self.allActors if x not in (self.cv[cv_index][0]+ self.cv[cv_index][1])]:\n",
    "                    for file in files:\n",
    "                        self.data.append([root[-8:],file[6:8],file])\n",
    "                        \n",
    "            elif split == 'validate':\n",
    "                if root[-8:] in self.cv[cv_index][0]:\n",
    "                    for file in files:\n",
    "                        self.data.append([root[-8:],file[6:8],file])\n",
    "                        \n",
    "            elif split == 'test':\n",
    "                if root[-8:] in self.cv[cv_index][1]:\n",
    "                    for file in files:\n",
    "                        self.data.append([root[-8:],file[6:8],file])\n",
    "            else:\n",
    "                print(\"Error: Split can only be train, validate or test!\")\n",
    "\n",
    "        # Convert data to pandas dataframe\n",
    "        self.df = pd.DataFrame(self.data, columns=['Speaker', 'Emotion', 'File'])\n",
    "\n",
    "        # Map emotion labels to numeric values\n",
    "        self.df['Emotion'] = self.df['Emotion'].map(self._emotions).astype(np.long)\n",
    "        self.df['Speaker'] = self.df['Speaker'].map(self._speaker).astype(np.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        audio_name = os.path.join(self.root, self.rev_speaker[self.df.loc[idx,'Speaker']], self.df.loc[idx, 'File'])\n",
    "        waveform, torch_sr = torchaudio.load(audio_name)\n",
    "        signal, _ = librosa.load(audio_name,sr=torch_sr)\n",
    "        trimmed_signal,index = librosa.effects.trim(signal,top_db = 25)\n",
    "        signal_wiener = scipy.signal.wiener(trimmed_signal)\n",
    "        signal_wiener = torch.from_numpy(signal_wiener)\n",
    "        signal_wiener = torch.unsqueeze(signal_wiener, 0)\n",
    "        signal_wiener = signal_wiener.type(torch.FloatTensor)\n",
    "        \n",
    "        if signal_wiener.shape[0] > 1:\n",
    "            signal_wiener = torch.unsqueeze(signal_wiener[0],0)\n",
    "        if signal_wiener.shape[1] <= 169472:\n",
    "            signal_wiener_padded = F.pad(input=signal_wiener, pad=(0, 169472 - signal_wiener.shape[1] , 0, 0), mode='constant', value=0)\n",
    "    \n",
    "        emotion = self.df.loc[idx, 'Emotion']\n",
    "        speaker = self.df.loc[idx, 'Speaker']\n",
    "        \n",
    "        # return a tuple instead of a dictionary\n",
    "        sample = (signal_wiener_padded,torch_sr,emotion,speaker)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022382,
     "end_time": "2021-02-05T14:17:11.746860",
     "exception": false,
     "start_time": "2021-02-05T14:17:11.724478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Example Creation of Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:11.797081Z",
     "iopub.status.busy": "2021-02-05T14:17:11.796514Z",
     "iopub.status.idle": "2021-02-05T14:17:11.798237Z",
     "shell.execute_reply": "2021-02-05T14:17:11.798660Z"
    },
    "papermill": {
     "duration": 0.026615,
     "end_time": "2021-02-05T14:17:11.798810",
     "exception": false,
     "start_time": "2021-02-05T14:17:11.772195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Initialize RavdessDataset\n",
    "# ravdess_dataset_train = RavdessDataset2('./Dataset/ravdess',cv_index = 4,split= 'train')\n",
    "# ravdess_dataset_test = RavdessDataset2('./Dataset/ravdess',cv_index = 4,split= 'test')\n",
    "# ravdess_dataset_validate = RavdessDataset2('./Dataset/ravdess',cv_index = 4, split= 'validate')\n",
    "\n",
    "# # To view dataframe, uncomment below: \n",
    "# # ravdess_dataset_train.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:11.844895Z",
     "iopub.status.busy": "2021-02-05T14:17:11.844334Z",
     "iopub.status.idle": "2021-02-05T14:17:11.846498Z",
     "shell.execute_reply": "2021-02-05T14:17:11.846039Z"
    },
    "papermill": {
     "duration": 0.026449,
     "end_time": "2021-02-05T14:17:11.846626",
     "exception": false,
     "start_time": "2021-02-05T14:17:11.820177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # To listen to 1st training audio file: \n",
    "# print(ravdess_dataset_train[0])\n",
    "\n",
    "# import IPython.display as ipd\n",
    "# torchaudio.backend.sox_backend.save(filepath = './Dataset/train_pad1.wav', src = ravdess_dataset_train[0][0], sample_rate = 48000)\n",
    "# ipd.Audio('./Dataset/train_pad1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:11.933457Z",
     "iopub.status.busy": "2021-02-05T14:17:11.932897Z",
     "iopub.status.idle": "2021-02-05T14:17:11.934526Z",
     "shell.execute_reply": "2021-02-05T14:17:11.934970Z"
    },
    "papermill": {
     "duration": 0.067638,
     "end_time": "2021-02-05T14:17:11.935126",
     "exception": false,
     "start_time": "2021-02-05T14:17:11.867488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# TRAIN_BATCH_SIZE = 32\n",
    "# VALIDATE_BATCH_SIZE = len(ravdess_dataset_validate)\n",
    "# TEST_BATCH_SIZE = len(ravdess_dataset_test)\n",
    "# ravdess_train_loader = DataLoader(dataset=ravdess_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=4, pin_memory=True)\n",
    "# ravdess_validate_loader = DataLoader(dataset=ravdess_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=4, pin_memory=True)\n",
    "# ravdess_test_loader = DataLoader(dataset=ravdess_dataset_test, batch_size= TEST_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:11.980523Z",
     "iopub.status.busy": "2021-02-05T14:17:11.979958Z",
     "iopub.status.idle": "2021-02-05T14:17:11.981679Z",
     "shell.execute_reply": "2021-02-05T14:17:11.982112Z"
    },
    "papermill": {
     "duration": 0.026181,
     "end_time": "2021-02-05T14:17:11.982266",
     "exception": false,
     "start_time": "2021-02-05T14:17:11.956085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Number of batches in:\")\n",
    "# print(\"Ravdess_train_loader: {}\".format(len(ravdess_train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:12.027723Z",
     "iopub.status.busy": "2021-02-05T14:17:12.027155Z",
     "iopub.status.idle": "2021-02-05T14:17:12.029389Z",
     "shell.execute_reply": "2021-02-05T14:17:12.028927Z"
    },
    "papermill": {
     "duration": 0.026101,
     "end_time": "2021-02-05T14:17:12.029524",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.003423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Number of batches in:\")\n",
    "# print(\"Ravdess_validate_loader: {}\".format(len(ravdess_validate_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:12.076052Z",
     "iopub.status.busy": "2021-02-05T14:17:12.075490Z",
     "iopub.status.idle": "2021-02-05T14:17:12.077290Z",
     "shell.execute_reply": "2021-02-05T14:17:12.077711Z"
    },
    "papermill": {
     "duration": 0.026755,
     "end_time": "2021-02-05T14:17:12.077880",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.051125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"Number of batches in:\")\n",
    "# print(\"Ravdess_test_loader: {}\".format(len(ravdess_test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021839,
     "end_time": "2021-02-05T14:17:12.120853",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.099014",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Network Architectures and Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020939,
     "end_time": "2021-02-05T14:17:12.162975",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.142036",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.1 Network Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:12.214291Z",
     "iopub.status.busy": "2021-02-05T14:17:12.213685Z",
     "iopub.status.idle": "2021-02-05T14:17:12.215916Z",
     "shell.execute_reply": "2021-02-05T14:17:12.215438Z"
    },
    "papermill": {
     "duration": 0.032123,
     "end_time": "2021-02-05T14:17:12.216049",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.183926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        # For same padding, P = ((S-1)*W-S+F)/2, with F = filter size, S = stride. If stride = 1, P = (F-1)/2\n",
    "        super().__init__()\n",
    "        self.one_layer_LFLB = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(2, 2),padding=1, bias=False),  \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout2d(p=0.1),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),  \n",
    "                     \n",
    "        ) \n",
    "        \n",
    "        self.GRU = nn.GRU(input_size= 4096, hidden_size = 256, batch_first = True, bidirectional = True) #128\n",
    "        self.global_average_pooling = nn.AdaptiveAvgPool2d((8,8))\n",
    "        self.FC = nn.Linear(in_features = 8 * 8, out_features = 512) \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.one_layer_LFLB(x)\n",
    "        # print(\"1-LFLB shape: {}\".format(x.shape))                     # batch_size, c_out, freq, time since we are using spectogram\n",
    "        x = x.view(x.size(0),-1,x.size(3)) \n",
    "        # print(\"Batch size, features, seq shape: {}\".format(x.shape))  # batch_size, features= c_out* freq , seq_len= time\n",
    "        x = x.permute(0,2,1)\n",
    "        # print(\"Batch size, seq, features shape: {}\".format(x.shape))  # batch_size, seq, features\n",
    "        output, hn = self.GRU(x)\n",
    "        # print(\"GRU output shape: {}\".format(output.shape))            \n",
    "        x = self.global_average_pooling(output)\n",
    "        # print(\"Shape after global average pooling:{}\".format(x.shape))\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        # print(\"Shape after flattening:{}\".format(x.shape))\n",
    "        x = self.FC(x)\n",
    "        # print(\"FC shape: {}\".format(x.shape))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:12.266460Z",
     "iopub.status.busy": "2021-02-05T14:17:12.265900Z",
     "iopub.status.idle": "2021-02-05T14:17:12.267638Z",
     "shell.execute_reply": "2021-02-05T14:17:12.268054Z"
    },
    "papermill": {
     "duration": 0.030101,
     "end_time": "2021-02-05T14:17:12.268214",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.238113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.label_classifier = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(512, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 8),\n",
    "\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.label_classifier(x)\n",
    "        return F.softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:12.316207Z",
     "iopub.status.busy": "2021-02-05T14:17:12.315621Z",
     "iopub.status.idle": "2021-02-05T14:17:12.317337Z",
     "shell.execute_reply": "2021-02-05T14:17:12.317751Z"
    },
    "papermill": {
     "duration": 0.029209,
     "end_time": "2021-02-05T14:17:12.317926",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.288717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SpeakerClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpeakerClassifier,self).__init__()\n",
    "        self.label_classifier = nn.Sequential(\n",
    "            nn.Linear(512, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 24),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.label_classifier(x)\n",
    "        return F.softmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020888,
     "end_time": "2021-02-05T14:17:12.361136",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.340248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.2 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:12.457651Z",
     "iopub.status.busy": "2021-02-05T14:17:12.457096Z",
     "iopub.status.idle": "2021-02-05T14:17:12.458983Z",
     "shell.execute_reply": "2021-02-05T14:17:12.459403Z"
    },
    "papermill": {
     "duration": 0.029158,
     "end_time": "2021-02-05T14:17:12.459560",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.430402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def LMS_Extractor(waveform, DEVICE):\n",
    "    x = torchaudio.transforms.MelSpectrogram(sample_rate = 48000,n_fft = 2048, hop_length = 512, power = 2).to(DEVICE)(waveform)\n",
    "    x = torchaudio.transforms.AmplitudeToDB()(x)\n",
    "    # print(x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:12.508789Z",
     "iopub.status.busy": "2021-02-05T14:17:12.508229Z",
     "iopub.status.idle": "2021-02-05T14:17:12.510025Z",
     "shell.execute_reply": "2021-02-05T14:17:12.510444Z"
    },
    "papermill": {
     "duration": 0.027772,
     "end_time": "2021-02-05T14:17:12.510599",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.482827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_parameters(PATH):\n",
    "    torch.save({\n",
    "                'encoder_state_dict': encoder.state_dict(),\n",
    "                'emotion_classifier_state_dict': emotion_classifier.state_dict(),\n",
    "                #'speaker_classifier_state_dict': speaker_classifier.state_dict(),\n",
    "                'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "                'emotion_optimizer_state_dict': emotion_optimizer.state_dict(),\n",
    "                #'speaker_optimizer_state_dict': speaker_optimizer.state_dict(),\n",
    "                }, PATH)\n",
    "    print(\"Models' parameters and optimisers' parameters saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021651,
     "end_time": "2021-02-05T14:17:12.553433",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.531782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:17:12.614472Z",
     "iopub.status.busy": "2021-02-05T14:17:12.613794Z",
     "iopub.status.idle": "2021-02-05T14:37:23.302166Z",
     "shell.execute_reply": "2021-02-05T14:37:23.301651Z"
    },
    "papermill": {
     "duration": 1210.727398,
     "end_time": "2021-02-05T14:37:23.302349",
     "exception": false,
     "start_time": "2021-02-05T14:17:12.574951",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch.optim as optim\n",
    "import pylab as plt\n",
    "\n",
    "# Read this to understand how GRL works: \n",
    "# https://christineai.blog/category/domain-adaptation/\n",
    "\n",
    "fold_emotion_class_losses = []\n",
    "fold_emotion_training_accuracies = []\n",
    "fold_emotion_validating_accuracies = []\n",
    "\n",
    "\n",
    "for fold in range(12):\n",
    "    \n",
    "    # Selecting CPU or GPU\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Selecting the type of encoder, label classifier\n",
    "    encoder = FeatureExtractor().to(DEVICE).train()\n",
    "    emotion_classifier = EmotionClassifier().to(DEVICE).train()\n",
    "    \n",
    "#     encoder = nn.DataParallel(encoder)\n",
    "#     emotion_classifier = nn.DataParallel(emotion_classifier)\n",
    "\n",
    "    # Optimizer \n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters())\n",
    "    emotion_optimizer = torch.optim.Adam(emotion_classifier.parameters())\n",
    "\n",
    "    cross_entropy_loss = nn.CrossEntropyLoss().to(DEVICE)\n",
    "    \n",
    "    ravdess_dataset_train = RavdessDataset2('./Dataset/ravdess',cv_index = fold ,split= 'train')\n",
    "    ravdess_dataset_validate = RavdessDataset2('./Dataset/ravdess',cv_index = fold, split= 'validate')\n",
    "    \n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    ravdess_train_loader = DataLoader(dataset=ravdess_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=True,worker_init_fn=np.random.seed(42),num_workers=4, pin_memory=True)\n",
    "    \n",
    "    # For evaluation purposes\n",
    "    VALIDATE_BATCH_SIZE = len(ravdess_dataset_validate)\n",
    "    ravdess_validate_loader = DataLoader(dataset=ravdess_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=4, pin_memory=True)\n",
    "    \n",
    "    epoch_emotion_class_losses = []\n",
    "    emotion_training_accuracies = []\n",
    "    emotion_validating_accuracies = []\n",
    "\n",
    "    EPOCHS = 100\n",
    "    STEP = 0\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(\"\\nCurrent Fold: {} | Epoch: {}\".format(fold, epoch))\n",
    "\n",
    "        batch_emotion_class_losses = []\n",
    "\n",
    "        for batch_idx, (waveform, sample_rate, emotion, speaker) in enumerate(ravdess_train_loader):\n",
    "\n",
    "            # Assigned to DEVICE. \n",
    "            waveform, emotion = waveform.to(DEVICE),emotion.to(DEVICE) \n",
    "\n",
    "            lms_features = LMS_Extractor(waveform , DEVICE)\n",
    "\n",
    "            # Calculate label classifier predictions on source batch\n",
    "            lms_conv_features = encoder(lms_features)\n",
    "            emotion_preds = emotion_classifier(lms_conv_features)\n",
    "            emotion_class_loss = cross_entropy_loss(emotion_preds, emotion)\n",
    "\n",
    "            # Clear the gradient to prevent gradient accumulation\n",
    "            encoder.zero_grad(set_to_none= True)\n",
    "            emotion_classifier.zero_grad(set_to_none= True)\n",
    "\n",
    "            # Computing the gradient\n",
    "            emotion_class_loss.backward()\n",
    "\n",
    "            # Update the weight\n",
    "            emotion_optimizer.step()\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "            batch_emotion_class_losses.append(emotion_class_loss)\n",
    "\n",
    "\n",
    "        # Enter evaluation mode at the end of each epoch\n",
    "        encoder.eval()\n",
    "        emotion_classifier.eval()\n",
    "\n",
    "        emotion_training_correct, emotion_validating_correct = 0 , 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "        # 1. Emotion (training) Classification\n",
    "          for index, (waveform, sample_rate, emotion, speaker) in enumerate(ravdess_train_loader):\n",
    "            waveform, emotion = waveform.to(DEVICE),emotion.to(DEVICE) \n",
    "            lms_features = LMS_Extractor(waveform , DEVICE)\n",
    "            # lms_features = torch.squeeze(lms_features).permute(0,2,1)\n",
    "            lms_conv_features = encoder(lms_features)\n",
    "            emotion_output = emotion_classifier(lms_conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            emotion_training_correct += (emotion_preds == emotion).sum() \n",
    "          #source_accuracy = torch.true_divide(source_correct, len(svhn_test_loader.dataset))\n",
    "          emotion_training_accuracy = emotion_training_correct.item()/(len(ravdess_train_loader)*TRAIN_BATCH_SIZE)\n",
    "          emotion_training_accuracies.append(emotion_training_accuracy)\n",
    "          print(\"Emotion Training Correct: {}/{} \\nEmotion Training Accuracy: {:.5f}%\".format(emotion_training_correct,(len(ravdess_train_loader)*TRAIN_BATCH_SIZE),emotion_training_accuracy*100))\n",
    "\n",
    "        # 2. Emotion (validating) Classification\n",
    "          for index, (waveform, sample_rate, emotion, speaker) in enumerate(ravdess_validate_loader):\n",
    "            waveform, emotion = waveform.to(DEVICE),emotion.to(DEVICE) \n",
    "            lms_features = LMS_Extractor(waveform , DEVICE)\n",
    "            # lms_features = torch.squeeze(lms_features).permute(0,2,1)\n",
    "            lms_conv_features = encoder(lms_features)\n",
    "            emotion_output = emotion_classifier(lms_conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            emotion_validating_correct += (emotion_preds == emotion).sum() \n",
    "          #source_accuracy = torch.true_divide(source_correct, len(svhn_test_loader.dataset))\n",
    "          emotion_validating_accuracy = emotion_validating_correct.item()/(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "          emotion_validating_accuracies.append(emotion_validating_accuracy)\n",
    "          print(\"Emotion Validating Correct: {}/{} \\nEmotion Validating Accuracy: {:.5f}%\".format(emotion_validating_correct,(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE),emotion_validating_accuracy*100))\n",
    "\n",
    "          if (len(emotion_validating_accuracies)> 1 and emotion_validating_accuracy >= max(emotion_validating_accuracies[:-1])):\n",
    "                save_parameters(MODEL_PATH + 'fold' + str(fold))\n",
    "\n",
    "\n",
    "        encoder.train()\n",
    "        emotion_classifier.train()\n",
    "\n",
    "        epoch_emotion_class_loss = torch.mean(torch.stack(batch_emotion_class_losses), dim=0)\n",
    "        epoch_emotion_class_losses.append(epoch_emotion_class_loss)\n",
    "        \n",
    "    \n",
    "        \n",
    "    fold_emotion_class_losses.append(epoch_emotion_class_losses)\n",
    "    fold_emotion_training_accuracies.append(emotion_training_accuracies)\n",
    "    fold_emotion_validating_accuracies.append(emotion_validating_accuracies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:37:23.483145Z",
     "iopub.status.busy": "2021-02-05T14:37:23.482358Z",
     "iopub.status.idle": "2021-02-05T14:37:23.484895Z",
     "shell.execute_reply": "2021-02-05T14:37:23.484412Z"
    },
    "papermill": {
     "duration": 0.095923,
     "end_time": "2021-02-05T14:37:23.485028",
     "exception": false,
     "start_time": "2021-02-05T14:37:23.389105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# np.savez(NPARR_PATH, \n",
    "#          emoClassLoss = fold_emotion_class_losses,\n",
    "#          emoTrain_acc = fold_emotion_training_accuracies, \n",
    "#          emoValidate_acc = fold_emotion_validating_accuracies )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:37:23.646347Z",
     "iopub.status.busy": "2021-02-05T14:37:23.645519Z",
     "iopub.status.idle": "2021-02-05T14:37:23.648322Z",
     "shell.execute_reply": "2021-02-05T14:37:23.647826Z"
    },
    "papermill": {
     "duration": 0.086574,
     "end_time": "2021-02-05T14:37:23.648461",
     "exception": false,
     "start_time": "2021-02-05T14:37:23.561887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,5))\n",
    "# plt.title('Losses vs. epochs')\n",
    "# plt.xlabel('epochs')\n",
    "# plt.ylabel('Losses')\n",
    "\n",
    "# for i in range(12):\n",
    "#     plt.plot(range(EPOCHS), fold_emotion_class_losses[i],label='emotion classification loss fold {}'.format(i))\n",
    "\n",
    "# plt.legend(loc='best')\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:37:23.808572Z",
     "iopub.status.busy": "2021-02-05T14:37:23.807616Z",
     "iopub.status.idle": "2021-02-05T14:37:23.810587Z",
     "shell.execute_reply": "2021-02-05T14:37:23.809780Z"
    },
    "papermill": {
     "duration": 0.083984,
     "end_time": "2021-02-05T14:37:23.810799",
     "exception": false,
     "start_time": "2021-02-05T14:37:23.726815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def plot_accuracies_vs_epochs(fold):\n",
    "#     plt.figure(figsize=(20,5))\n",
    "#     plt.title('Accuracies vs. epochs')\n",
    "#     plt.xlabel('epochs')\n",
    "#     plt.ylabel('Losses')\n",
    "#     plt.plot(range(EPOCHS), fold_emotion_training_accuracies[fold],label='emotion_training_accuracies fold {}'.format(fold))\n",
    "#     plt.plot(range(EPOCHS), fold_emotion_validating_accuracies[fold],label='emotion_validating_accuracies fold {}'.format(fold))\n",
    "#     plt.legend(loc='best')\n",
    "#     plt.show() \n",
    "#     print(\"Maximum emotion training accuracy:{:.2f}%\".format(max(fold_emotion_training_accuracies[fold])*100))\n",
    "#     print(\"Maximum emotion validating accuracy:{:.2f}%\".format(max(fold_emotion_validating_accuracies[fold])*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:37:24.015555Z",
     "iopub.status.busy": "2021-02-05T14:37:24.014933Z",
     "iopub.status.idle": "2021-02-05T14:37:24.016682Z",
     "shell.execute_reply": "2021-02-05T14:37:24.017185Z"
    },
    "papermill": {
     "duration": 0.082661,
     "end_time": "2021-02-05T14:37:24.017361",
     "exception": false,
     "start_time": "2021-02-05T14:37:23.934700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for i in range(12):\n",
    "#     plot_accuracies_vs_epochs(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.079005,
     "end_time": "2021-02-05T14:37:24.182060",
     "exception": false,
     "start_time": "2021-02-05T14:37:24.103055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Loading and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-05T14:37:24.358742Z",
     "iopub.status.busy": "2021-02-05T14:37:24.341040Z",
     "iopub.status.idle": "2021-02-05T15:18:19.687502Z",
     "shell.execute_reply": "2021-02-05T15:18:19.688345Z"
    },
    "papermill": {
     "duration": 2455.429346,
     "end_time": "2021-02-05T15:18:19.688703",
     "exception": false,
     "start_time": "2021-02-05T14:37:24.259357",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation for fold 0\n",
      "Emotion Training Correct: 808/1200 \n",
      "Emotion Training Accuracy: 67.33333%\n",
      "Emotion Validating Correct: 70/120 \n",
      "Emotion Validating Accuracy: 58.33333%\n",
      "Emotion Testing Correct: 55/120 \n",
      "Emotion Testing Accuracy: 45.83333%\n",
      "\n",
      "Evaluation for fold 1\n",
      "Emotion Training Correct: 655/1200 \n",
      "Emotion Training Accuracy: 54.58333%\n",
      "Emotion Validating Correct: 61/120 \n",
      "Emotion Validating Accuracy: 50.83333%\n",
      "Emotion Testing Correct: 46/120 \n",
      "Emotion Testing Accuracy: 38.33333%\n",
      "\n",
      "Evaluation for fold 2\n",
      "Emotion Training Correct: 732/1200 \n",
      "Emotion Training Accuracy: 61.00000%\n",
      "Emotion Validating Correct: 63/120 \n",
      "Emotion Validating Accuracy: 52.50000%\n",
      "Emotion Testing Correct: 63/120 \n",
      "Emotion Testing Accuracy: 52.50000%\n",
      "\n",
      "Evaluation for fold 3\n",
      "Emotion Training Correct: 863/1200 \n",
      "Emotion Training Accuracy: 71.91667%\n",
      "Emotion Validating Correct: 75/120 \n",
      "Emotion Validating Accuracy: 62.50000%\n",
      "Emotion Testing Correct: 44/120 \n",
      "Emotion Testing Accuracy: 36.66667%\n",
      "\n",
      "Evaluation for fold 4\n",
      "Emotion Training Correct: 916/1200 \n",
      "Emotion Training Accuracy: 76.33333%\n",
      "Emotion Validating Correct: 49/120 \n",
      "Emotion Validating Accuracy: 40.83333%\n",
      "Emotion Testing Correct: 62/120 \n",
      "Emotion Testing Accuracy: 51.66667%\n",
      "\n",
      "Evaluation for fold 5\n",
      "Emotion Training Correct: 914/1200 \n",
      "Emotion Training Accuracy: 76.16667%\n",
      "Emotion Validating Correct: 69/120 \n",
      "Emotion Validating Accuracy: 57.50000%\n",
      "Emotion Testing Correct: 44/120 \n",
      "Emotion Testing Accuracy: 36.66667%\n",
      "\n",
      "Evaluation for fold 6\n",
      "Emotion Training Correct: 838/1200 \n",
      "Emotion Training Accuracy: 69.83333%\n",
      "Emotion Validating Correct: 47/120 \n",
      "Emotion Validating Accuracy: 39.16667%\n",
      "Emotion Testing Correct: 51/120 \n",
      "Emotion Testing Accuracy: 42.50000%\n",
      "\n",
      "Evaluation for fold 7\n",
      "Emotion Training Correct: 968/1200 \n",
      "Emotion Training Accuracy: 80.66667%\n",
      "Emotion Validating Correct: 67/120 \n",
      "Emotion Validating Accuracy: 55.83333%\n",
      "Emotion Testing Correct: 44/120 \n",
      "Emotion Testing Accuracy: 36.66667%\n",
      "\n",
      "Evaluation for fold 8\n",
      "Emotion Training Correct: 884/1200 \n",
      "Emotion Training Accuracy: 73.66667%\n",
      "Emotion Validating Correct: 59/120 \n",
      "Emotion Validating Accuracy: 49.16667%\n",
      "Emotion Testing Correct: 55/120 \n",
      "Emotion Testing Accuracy: 45.83333%\n",
      "\n",
      "Evaluation for fold 9\n",
      "Emotion Training Correct: 834/1200 \n",
      "Emotion Training Accuracy: 69.50000%\n",
      "Emotion Validating Correct: 56/120 \n",
      "Emotion Validating Accuracy: 46.66667%\n",
      "Emotion Testing Correct: 44/120 \n",
      "Emotion Testing Accuracy: 36.66667%\n",
      "\n",
      "Evaluation for fold 10\n",
      "Emotion Training Correct: 820/1200 \n",
      "Emotion Training Accuracy: 68.33333%\n",
      "Emotion Validating Correct: 55/120 \n",
      "Emotion Validating Accuracy: 45.83333%\n",
      "Emotion Testing Correct: 56/120 \n",
      "Emotion Testing Accuracy: 46.66667%\n",
      "\n",
      "Evaluation for fold 11\n",
      "Emotion Training Correct: 720/1200 \n",
      "Emotion Training Accuracy: 60.00000%\n",
      "Emotion Validating Correct: 58/120 \n",
      "Emotion Validating Accuracy: 48.33333%\n",
      "Emotion Testing Correct: 60/120 \n",
      "Emotion Testing Accuracy: 50.00000%\n",
      "\n",
      "SUMMARY:\n",
      "\n",
      "CV Emotion Training accuracies \n",
      "Mean: 0.6911111111111111 \n",
      "S.D: 0.07558219598667633\n",
      "\n",
      "CV Emotion Validating accuracies \n",
      "Mean: 0.50625 \n",
      "S.D: 0.07081104818779727\n",
      "\n",
      "CV Emotion Testing accuracies \n",
      "Mean: 0.43333333333333335 \n",
      "S.D: 0.06215815605080612\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean , stdev\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lfold_emotion_training_accuracies, lfold_emotion_validating_accuracies, lfold_emotion_testing_accuracies = [] , [] , []\n",
    "\n",
    "for fold in range(12):\n",
    "    print(\"\\nEvaluation for fold {}\".format(fold))\n",
    "    checkpoint = torch.load(MODEL_PATH + 'fold' + str(fold))\n",
    "    \n",
    "    # DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "    encoder = FeatureExtractor().to(DEVICE).train()\n",
    "    emotion_classifier = EmotionClassifier().to(DEVICE).train()\n",
    "    #speaker_classifier = SpeakerClassifier().to(DEVICE)\n",
    "    \n",
    "#     encoder = nn.DataParallel(encoder)\n",
    "#     emotion_classifier = nn.DataParallel(emotion_classifier)\n",
    "\n",
    "    encoder_optimizer = torch.optim.Adam(encoder.parameters())\n",
    "    emotion_optimizer = torch.optim.Adam(emotion_classifier.parameters())\n",
    "    #speaker_optimizer = torch.optim.Adam(speaker_classifier.parameters())\n",
    "    \n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    emotion_classifier.load_state_dict(checkpoint['emotion_classifier_state_dict'])\n",
    "    #speaker_classifier.load_state_dict(checkpoint['speaker_classifier_state_dict'])\n",
    "    \n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
    "    emotion_optimizer.load_state_dict(checkpoint['emotion_optimizer_state_dict'])\n",
    "    #speaker_optimizer.load_state_dict(checkpoint['speaker_optimizer_state_dict'])\n",
    "    \n",
    "    ravdess_dataset_train = RavdessDataset2('./Dataset/ravdess',cv_index = fold ,split= 'train')\n",
    "    ravdess_dataset_test = RavdessDataset2('./Dataset/ravdess',cv_index = fold, split= 'test')\n",
    "    ravdess_dataset_validate = RavdessDataset2('./Dataset/ravdess',cv_index = fold, split= 'validate')\n",
    "    \n",
    "    TRAIN_BATCH_SIZE = len(ravdess_dataset_train)\n",
    "    VALIDATE_BATCH_SIZE = len(ravdess_dataset_validate)\n",
    "    TEST_BATCH_SIZE = len(ravdess_dataset_test)\n",
    "    \n",
    "    ravdess_train_loader = DataLoader(dataset=ravdess_dataset_train, batch_size= TRAIN_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    ravdess_validate_loader = DataLoader(dataset=ravdess_dataset_validate, batch_size= VALIDATE_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    ravdess_test_loader = DataLoader(dataset=ravdess_dataset_test, batch_size= TEST_BATCH_SIZE, shuffle=True, drop_last=False,worker_init_fn=np.random.seed(42),num_workers=0)\n",
    "    \n",
    "    encoder.eval()\n",
    "    emotion_classifier.eval()\n",
    "\n",
    "    lemotion_training_correct, lemotion_validating_correct, lemotion_testing_correct = 0 , 0 , 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "\n",
    "        # 1. Training Classification\n",
    "        for index, (waveform, sample_rate, emotion, speaker) in enumerate(ravdess_train_loader):\n",
    "            waveform, emotion = waveform.to(DEVICE),emotion.to(DEVICE) \n",
    "            lms_features = LMS_Extractor(waveform , DEVICE)\n",
    "            lms_conv_features = encoder(lms_features)\n",
    "            emotion_output = emotion_classifier(lms_conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            lemotion_training_correct += (emotion_preds == emotion).sum() \n",
    "        emotion_training_accuracy = lemotion_training_correct.item()/(len(ravdess_train_loader)*TRAIN_BATCH_SIZE)\n",
    "        print(\"Emotion Training Correct: {}/{} \\nEmotion Training Accuracy: {:.5f}%\".format(lemotion_training_correct,(len(ravdess_train_loader)*TRAIN_BATCH_SIZE),emotion_training_accuracy*100))\n",
    "\n",
    "        # 2. Validating Classification\n",
    "        for index, (waveform, sample_rate, emotion, speaker) in enumerate(ravdess_validate_loader):\n",
    "            waveform, emotion = waveform.to(DEVICE),emotion.to(DEVICE) \n",
    "            lms_features = LMS_Extractor(waveform , DEVICE)\n",
    "            lms_conv_features = encoder(lms_features)\n",
    "            emotion_output = emotion_classifier(lms_conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            lemotion_validating_correct += (emotion_preds == emotion).sum() \n",
    "        emotion_validating_accuracy = lemotion_validating_correct.item()/(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE)\n",
    "        print(\"Emotion Validating Correct: {}/{} \\nEmotion Validating Accuracy: {:.5f}%\".format(lemotion_validating_correct,(len(ravdess_validate_loader)*VALIDATE_BATCH_SIZE),emotion_validating_accuracy*100))        \n",
    "\n",
    "\n",
    "        # 3. Testing Classification\n",
    "        for index, (waveform, sample_rate, emotion, speaker) in enumerate(ravdess_test_loader):\n",
    "            waveform, emotion = waveform.to(DEVICE),emotion.to(DEVICE) \n",
    "            lms_features = LMS_Extractor(waveform , DEVICE)\n",
    "            lms_conv_features = encoder(lms_features)\n",
    "            emotion_output = emotion_classifier(lms_conv_features)\n",
    "            _, emotion_preds = torch.max(emotion_output,1)\n",
    "            lemotion_testing_correct += (emotion_preds == emotion).sum() \n",
    "        emotion_testing_accuracy = lemotion_testing_correct.item()/(len(ravdess_test_loader)*TEST_BATCH_SIZE)\n",
    "        print(\"Emotion Testing Correct: {}/{} \\nEmotion Testing Accuracy: {:.5f}%\".format(lemotion_testing_correct,(len(ravdess_test_loader)*TEST_BATCH_SIZE),emotion_testing_accuracy*100))\n",
    "        \n",
    "        lfold_emotion_training_accuracies.append(emotion_training_accuracy)\n",
    "        lfold_emotion_validating_accuracies.append(emotion_validating_accuracy)\n",
    "        lfold_emotion_testing_accuracies.append(emotion_testing_accuracy)\n",
    "    \n",
    "\n",
    "print('\\nSUMMARY:')\n",
    "print('\\nCV Emotion Training accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_training_accuracies), stdev(lfold_emotion_training_accuracies)))\n",
    "print('\\nCV Emotion Validating accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_validating_accuracies), stdev(lfold_emotion_validating_accuracies)))\n",
    "print('\\nCV Emotion Testing accuracies \\nMean: {} \\nS.D: {}'.format(mean(lfold_emotion_testing_accuracies), stdev(lfold_emotion_testing_accuracies)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3690.766167,
   "end_time": "2021-02-05T15:18:26.958743",
   "environment_variables": {},
   "exception": null,
   "input_path": "[NN] RAVDESS_LMS_1L-CNN-GRU_32 training batch size.ipynb",
   "output_path": "[NN] RAVDESS_LMS_1L-CNN-GRU_32 training batch size.ipynb",
   "parameters": {},
   "start_time": "2021-02-05T14:16:56.192576",
   "version": "2.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
