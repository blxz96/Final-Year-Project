{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE = False\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import scipy\n",
    "from scipy import signal\n",
    "import librosa.display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractors for emoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import MFCC\n",
    "\n",
    "def MFCC_Extractor(waveform, DEVICE):\n",
    "    x = MFCC(sample_rate=16000, \n",
    "             n_mfcc=20,\n",
    "             melkwargs={\"n_fft\": 2048, \"hop_length\": 512, \"power\": 2}).to(DEVICE)(waveform)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan/miniconda3/envs/torch2/lib/python3.8/site-packages/scipy/signal/signaltools.py:1598: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  res *= (1 - noise / lVar)\n",
      "/home/bryan/miniconda3/envs/torch2/lib/python3.8/site-packages/scipy/signal/signaltools.py:1598: RuntimeWarning: invalid value encountered in multiply\n",
      "  res *= (1 - noise / lVar)\n"
     ]
    }
   ],
   "source": [
    "root = './Dataset/emodb'\n",
    "target_location = './Dataset/emodb_MFCC'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        audio = root + '/' + file\n",
    "        waveform, torch_sr = torchaudio.load(audio)\n",
    "        signal, _ = librosa.load(audio,sr=torch_sr)\n",
    "        trimmed_signal,index = librosa.effects.trim(signal,top_db = 25)\n",
    "        signal_wiener = scipy.signal.wiener(trimmed_signal)\n",
    "        signal_wiener = torch.from_numpy(signal_wiener)\n",
    "        signal_wiener = torch.unsqueeze(signal_wiener, 0)\n",
    "        signal_wiener = signal_wiener.type(torch.FloatTensor)\n",
    "        if signal_wiener.shape[1] <= 143652:\n",
    "            signal_wiener_padded = F.pad(input=signal_wiener, \n",
    "                                         pad=(0, 143652 - signal_wiener.shape[1] , 0, 0), \n",
    "                                         mode='constant', value=0)\n",
    "        mfcc = MFCC_Extractor(signal_wiener_padded, DEVICE)\n",
    "        file_name = file[:-4]\n",
    "        torch.save(mfcc, target_location + '/' + file_name + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LMS_Extractor(waveform, DEVICE):\n",
    "    x = torchaudio.transforms.MelSpectrogram(sample_rate = 16000,\n",
    "                                             n_fft = 2048, \n",
    "                                             hop_length = 512, \n",
    "                                             power = 2).to(DEVICE)(waveform)\n",
    "    x = torchaudio.transforms.AmplitudeToDB()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './Dataset/emodb'\n",
    "target_location = './Dataset/emodb_LMS'\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        audio = root + '/' + file\n",
    "        waveform, torch_sr = torchaudio.load(audio)\n",
    "        signal, _ = librosa.load(audio,sr=torch_sr)\n",
    "        trimmed_signal,index = librosa.effects.trim(signal,top_db = 25)\n",
    "        signal_wiener = scipy.signal.wiener(trimmed_signal)\n",
    "        signal_wiener = torch.from_numpy(signal_wiener)\n",
    "        signal_wiener = torch.unsqueeze(signal_wiener, 0)\n",
    "        signal_wiener = signal_wiener.type(torch.FloatTensor)\n",
    "        if signal_wiener.shape[1] <= 143652:\n",
    "            signal_wiener_padded = F.pad(input=signal_wiener, pad=(0, 143652 - signal_wiener.shape[1] , 0, 0), mode='constant', value=0)\n",
    "        lms = LMS_Extractor(signal_wiener_padded, DEVICE)\n",
    "        file_name = file[:-4]\n",
    "        torch.save(lms, target_location + '/' + file_name + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.compliance.kaldi import fbank\n",
    "from torchaudio.functional import compute_kaldi_pitch\n",
    "def LogMFB_Energy_Pitch_NCCF_Extractor(waveform,sr):\n",
    "    logmfb_w_energy = fbank(waveform = waveform, sample_frequency=sr, frame_length=40, frame_shift=10,num_mel_bins=40,use_energy=True)\n",
    "    logmfb_w_energy = torch.unsqueeze(logmfb_w_energy,0)\n",
    "    pitch = compute_kaldi_pitch(waveform = waveform, sample_rate = sr, frame_length=40, frame_shift=10)\n",
    "    x = torch.cat((logmfb_w_energy,pitch),2).permute(0,2,1)\n",
    "    x = x.squeeze()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './Dataset/emodb'\n",
    "target_location = './Dataset/emodb_LogMFB_Energy_Pitch_NCCF'\n",
    "\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        audio = root + '/' + file\n",
    "        waveform, torch_sr = torchaudio.load(audio)\n",
    "        signal, _ = librosa.load(audio,sr=torch_sr)\n",
    "        trimmed_signal,index = librosa.effects.trim(signal,top_db = 25)\n",
    "        signal_wiener = scipy.signal.wiener(trimmed_signal)\n",
    "        signal_wiener = torch.from_numpy(signal_wiener)\n",
    "        signal_wiener = torch.unsqueeze(signal_wiener, 0)\n",
    "        signal_wiener = signal_wiener.type(torch.FloatTensor)\n",
    "        if signal_wiener.shape[1] <= 143652:\n",
    "            signal_wiener_padded = F.pad(input=signal_wiener, pad=(0, 143652 - signal_wiener.shape[1] , 0, 0), mode='constant', value=0)\n",
    "        logmfb_w_energy_pitch = LogMFB_Energy_Pitch_NCCF_Extractor(signal_wiener_padded,torch_sr)\n",
    "        file_name = file[:-4]\n",
    "        torch.save(logmfb_w_energy_pitch, target_location + '/' + file_name + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC and LMS Generation for RAVDESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MFCC_Extractor(waveform, DEVICE):\n",
    "    x = MFCC(sample_rate=48000, n_mfcc=20,melkwargs={\"n_fft\": 2048, \"hop_length\": 512, \"power\": 2}).to(DEVICE)(waveform)\n",
    "    # print(x.shape)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './Dataset/ravdess'\n",
    "target_location = './Dataset/ravdess_MFCC'\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        audio = root + '/' + file\n",
    "        waveform, torch_sr = torchaudio.load(audio)\n",
    "        signal, _ = librosa.load(audio,sr=torch_sr)\n",
    "        trimmed_signal,index = librosa.effects.trim(signal,top_db = 25)\n",
    "        signal_wiener = scipy.signal.wiener(trimmed_signal)\n",
    "        signal_wiener = torch.from_numpy(signal_wiener)\n",
    "        signal_wiener = torch.unsqueeze(signal_wiener, 0)\n",
    "        signal_wiener = signal_wiener.type(torch.FloatTensor)\n",
    "        if signal_wiener.shape[0] > 1:\n",
    "            signal_wiener = torch.unsqueeze(signal_wiener[0],0)\n",
    "        if signal_wiener.shape[1] <= 169472:\n",
    "            signal_wiener_padded = F.pad(input=signal_wiener, pad=(0, 169472 - signal_wiener.shape[1] , 0, 0), mode='constant', value=0)\n",
    "        mfcc = MFCC_Extractor(signal_wiener_padded, DEVICE)\n",
    "        file_name = file[:-4]\n",
    "        torch.save(mfcc, target_location + '/' + file_name + '.pt')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LMS_Extractor(waveform, DEVICE):\n",
    "    x = torchaudio.transforms.MelSpectrogram(sample_rate = 48000,n_fft = 2048, hop_length = 512, power = 2).to(DEVICE)(waveform)\n",
    "    x = torchaudio.transforms.AmplitudeToDB()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './Dataset/ravdess'\n",
    "target_location = './Dataset/ravdess_LMS'\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        audio = root + '/' + file\n",
    "        waveform, torch_sr = torchaudio.load(audio)\n",
    "        signal, _ = librosa.load(audio,sr=torch_sr)\n",
    "        trimmed_signal,index = librosa.effects.trim(signal,top_db = 25)\n",
    "        signal_wiener = scipy.signal.wiener(trimmed_signal)\n",
    "        signal_wiener = torch.from_numpy(signal_wiener)\n",
    "        signal_wiener = torch.unsqueeze(signal_wiener, 0)\n",
    "        signal_wiener = signal_wiener.type(torch.FloatTensor)\n",
    "        if signal_wiener.shape[0] > 1:\n",
    "            signal_wiener = torch.unsqueeze(signal_wiener[0],0)\n",
    "        if signal_wiener.shape[1] <= 169472:\n",
    "            signal_wiener_padded = F.pad(input=signal_wiener, pad=(0, 169472 - signal_wiener.shape[1] , 0, 0), mode='constant', value=0)\n",
    "        lms = LMS_Extractor(signal_wiener_padded, DEVICE)\n",
    "        file_name = file[:-4]\n",
    "        torch.save(lms, target_location + '/' + file_name + '.pt')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.compliance.kaldi import fbank\n",
    "from torchaudio.functional import compute_kaldi_pitch\n",
    "def LogMFB_Energy_Pitch_NCCF_Extractor(waveform,sr):\n",
    "    logmfb_w_energy = fbank(waveform = waveform, sample_frequency=sr, frame_length=40, frame_shift=10,num_mel_bins=40,use_energy=True)\n",
    "    logmfb_w_energy = torch.unsqueeze(logmfb_w_energy,0)\n",
    "    pitch = compute_kaldi_pitch(waveform = waveform, sample_rate = sr, frame_length=40, frame_shift=10)\n",
    "    x = torch.cat((logmfb_w_energy,pitch),2).permute(0,2,1)\n",
    "    x = x.squeeze()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './Dataset/ravdess'\n",
    "target_location = './Dataset/ravdess_LogMFB_Energy_Pitch_NCCF'\n",
    "\n",
    "for root, dirs, files in os.walk(root):\n",
    "    for file in files:\n",
    "        audio = root + '/' + file\n",
    "        waveform, torch_sr = torchaudio.load(audio)\n",
    "        signal, _ = librosa.load(audio,sr=torch_sr)\n",
    "        trimmed_signal,index = librosa.effects.trim(signal,top_db = 25)\n",
    "        signal_wiener = scipy.signal.wiener(trimmed_signal)\n",
    "        signal_wiener = torch.from_numpy(signal_wiener)\n",
    "        signal_wiener = torch.unsqueeze(signal_wiener, 0)\n",
    "        signal_wiener = signal_wiener.type(torch.FloatTensor)\n",
    "        if signal_wiener.shape[0] > 1:\n",
    "            signal_wiener = torch.unsqueeze(signal_wiener[0],0)\n",
    "        if signal_wiener.shape[1] <= 169472:\n",
    "            signal_wiener_padded = F.pad(input=signal_wiener, pad=(0, 169472 - signal_wiener.shape[1] , 0, 0), mode='constant', value=0)\n",
    "        logmfb_w_energy_pitch = LogMFB_Energy_Pitch_NCCF_Extractor(signal_wiener_padded,torch_sr)\n",
    "        file_name = file[:-4]\n",
    "        torch.save(logmfb_w_energy_pitch, target_location + '/' + file_name + '.pt')    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
